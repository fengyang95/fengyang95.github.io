{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用神经网络解决Titanic问题，主要用两种实现，一种是纯pytorch实现神经网络的搭建，另一种是使用skorch包装好的以pytorch为后端的API实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据文件，这里省去了特征工程的部分，特征采用之前特征工程提取出的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_None</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex  Title  Family_size  Family_Survival  \\\n",
       "0            1       3    0      0            2              0.5   \n",
       "1            2       1    1      2            2              0.5   \n",
       "2            3       3    1      1            1              0.5   \n",
       "3            4       1    1      2            2              0.0   \n",
       "4            5       3    0      0            1              0.5   \n",
       "\n",
       "   FareBin_Code  AgeBin_Code  Embarked_C  Embarked_None  Embarked_Q  \\\n",
       "0             0            2           0              0           0   \n",
       "1             4            3           1              0           0   \n",
       "2             1            2           0              0           0   \n",
       "3             4            3           0              0           0   \n",
       "4             1            3           0              0           0   \n",
       "\n",
       "   Embarked_S  Survived  \n",
       "0           1       0.0  \n",
       "1           0       1.0  \n",
       "2           1       1.0  \n",
       "3           1       1.0  \n",
       "4           1       0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('train_features.csv')\n",
    "test=pd.read_csv('test_features.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId        891 non-null int64\n",
      "Pclass             891 non-null int64\n",
      "Sex                891 non-null int64\n",
      "Title              891 non-null int64\n",
      "Family_size        891 non-null int64\n",
      "Family_Survival    891 non-null float64\n",
      "FareBin_Code       891 non-null int64\n",
      "AgeBin_Code        891 non-null int64\n",
      "Embarked_C         891 non-null int64\n",
      "Embarked_None      891 non-null int64\n",
      "Embarked_Q         891 non-null int64\n",
      "Embarked_S         891 non-null int64\n",
      "Survived           891 non-null float64\n",
      "dtypes: float64(2), int64(11)\n",
      "memory usage: 90.6 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_None</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>1.904602</td>\n",
       "      <td>0.519641</td>\n",
       "      <td>1.985410</td>\n",
       "      <td>2.433221</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.972450</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>1.411355</td>\n",
       "      <td>1.370957</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.047351</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.486592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Sex       Title  Family_size  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000   891.000000   \n",
       "mean    446.000000    2.308642    0.352413    0.686869     1.904602   \n",
       "std     257.353842    0.836071    0.477990    0.972450     1.613459   \n",
       "min       1.000000    1.000000    0.000000    0.000000     1.000000   \n",
       "25%     223.500000    2.000000    0.000000    0.000000     1.000000   \n",
       "50%     446.000000    3.000000    0.000000    0.000000     1.000000   \n",
       "75%     668.500000    3.000000    1.000000    1.000000     2.000000   \n",
       "max     891.000000    3.000000    1.000000    4.000000    11.000000   \n",
       "\n",
       "       Family_Survival  FareBin_Code  AgeBin_Code  Embarked_C  Embarked_None  \\\n",
       "count       891.000000    891.000000   891.000000  891.000000     891.000000   \n",
       "mean          0.519641      1.985410     2.433221    0.188552       0.002245   \n",
       "std           0.323961      1.411355     1.370957    0.391372       0.047351   \n",
       "min           0.000000      0.000000     0.000000    0.000000       0.000000   \n",
       "25%           0.500000      1.000000     2.000000    0.000000       0.000000   \n",
       "50%           0.500000      2.000000     2.000000    0.000000       0.000000   \n",
       "75%           0.500000      3.000000     3.000000    0.000000       0.000000   \n",
       "max           1.000000      4.000000     7.000000    1.000000       1.000000   \n",
       "\n",
       "       Embarked_Q  Embarked_S    Survived  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean     0.086420    0.722783    0.383838  \n",
       "std      0.281141    0.447876    0.486592  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    1.000000    0.000000  \n",
       "75%      0.000000    1.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.drop(['PassengerId','Survived'],axis=1).as_matrix()\n",
    "Y_train=train['Survived'].astype(int).as_matrix()\n",
    "\n",
    "X_test=test.drop(['PassengerId','Survived'],axis=1).as_matrix()\n",
    "IDtest=test['PassengerId']\n",
    "\n",
    "# scalar\n",
    "scaler=MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Pytorch搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据载入和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicFeaturesDataset(Dataset):\n",
    "    def __init__(self,X,y=None,transform=None):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.y is not None:\n",
    "            sample={'X':self.X[idx],'y':self.y[idx]}\n",
    "        else:\n",
    "            sample={'X':self.X[idx]}\n",
    "        if self.transform is not None:\n",
    "            sample=self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform class\n",
    "class ToTensor(object):\n",
    "    def __call__(self,sample):\n",
    "        if 'y' in sample.keys():\n",
    "            X,y=sample['X'],sample['y']\n",
    "            return {\n",
    "                'X':torch.from_numpy(X.astype(np.float32)),\n",
    "                'y':torch.squeeze(torch.from_numpy(np.array([y.astype(np.int64)])))\n",
    "            }\n",
    "        else:\n",
    "            X=sample['X']\n",
    "            return {\n",
    "                'X':torch.from_numpy(X.astype(np.float32))\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "# 自定义Model需要继承nn.Module\n",
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(self,D_in=11,D_out=2,num_units=20,nonlin=F.relu,dropout=0.5):\n",
    "        super(ClassifierModule,self).__init__()\n",
    "        self.num_units=num_units\n",
    "        self.nonlin=nonlin\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear1=nn.Linear(D_in,num_units)\n",
    "        self.linear2=nn.Linear(num_units,10)\n",
    "        self.output=nn.Linear(10,2)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.nonlin(self.linear1(X))\n",
    "        X=self.dropout(X)\n",
    "        X=self.nonlin(self.linear2(X))\n",
    "        X=self.dropout(X)\n",
    "        X=F.softmax(self.output(X),dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train_model(model,criterion,optimiizer,scheduler,dataset_sizes,num_epochs=100,device='cpu'):\n",
    "    start=time.time()\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    best_acc=0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss=0.0\n",
    "            running_corrects=0\n",
    "            for sample_batches in dataloaders[phase]:\n",
    "                inputs=sample_batches['X'].to(device)\n",
    "                labels=sample_batches['y'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs=model(inputs)\n",
    "                    _,preds=torch.max(outputs,1)\n",
    "                    loss=criterion(outputs,labels)\n",
    "                    \n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    running_loss+=loss.item()*inputs.size(0)\n",
    "                    running_corrects+=torch.sum(preds==labels.data)\n",
    "            epoch_loss=running_loss/dataset_sizes[phase]\n",
    "            epoch_acc=running_corrects.double()/dataset_sizes[phase]\n",
    "            print('{} loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss,epoch_acc))\n",
    "            \n",
    "            if phase=='val' and epoch_acc>best_acc:\n",
    "                best_acc=epoch_acc\n",
    "                best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    time_elapsed=time.time()-start\n",
    "    print('Trainning complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed//60,time_elapsed%60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and train\n",
    "train_dataset_len=X_train.shape[0]\n",
    "train_len=train_dataset_len*4//5\n",
    "\n",
    "transformed_datasets = {\n",
    "        'train': TitanicFeaturesDataset(X_train[:train_len],Y_train[:train_len], transform=transforms.Compose([ToTensor()])),\n",
    "        'val': TitanicFeaturesDataset(X_train[train_len:], Y_train[train_len:], transform=transforms.Compose([ToTensor()])),\n",
    "        'test':TitanicFeaturesDataset(X_test,transform=transforms.Compose([ToTensor()]))\n",
    "        }\n",
    "\n",
    "dataloaders = {x: DataLoader(transformed_datasets[x], batch_size=16,\n",
    "                                 shuffle=True, num_workers=0)\n",
    "                   for x in ['train', 'val']}\n",
    "dataloaders['test']=DataLoader(transformed_datasets['test'],batch_size=16,shuffle=False,num_workers=0)\n",
    "\n",
    "dataset_sizes = {x: len(transformed_datasets[x]) for x in ['train', 'val','test']}\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train loss: 0.6819 Acc: 0.5674\n",
      "val loss: 0.6576 Acc: 0.6425\n",
      "Epoch 1/99\n",
      "----------\n",
      "train loss: 0.6624 Acc: 0.6096\n",
      "val loss: 0.6418 Acc: 0.6425\n",
      "Epoch 2/99\n",
      "----------\n",
      "train loss: 0.6570 Acc: 0.6096\n",
      "val loss: 0.6249 Acc: 0.6425\n",
      "Epoch 3/99\n",
      "----------\n",
      "train loss: 0.6463 Acc: 0.6096\n",
      "val loss: 0.6069 Acc: 0.6425\n",
      "Epoch 4/99\n",
      "----------\n",
      "train loss: 0.6344 Acc: 0.6110\n",
      "val loss: 0.5795 Acc: 0.6425\n",
      "Epoch 5/99\n",
      "----------\n",
      "train loss: 0.6212 Acc: 0.6236\n",
      "val loss: 0.5534 Acc: 0.7933\n",
      "Epoch 6/99\n",
      "----------\n",
      "train loss: 0.6010 Acc: 0.6966\n",
      "val loss: 0.5272 Acc: 0.8380\n",
      "Epoch 7/99\n",
      "----------\n",
      "train loss: 0.5842 Acc: 0.7177\n",
      "val loss: 0.5245 Acc: 0.8380\n",
      "Epoch 8/99\n",
      "----------\n",
      "train loss: 0.5920 Acc: 0.7233\n",
      "val loss: 0.5218 Acc: 0.8380\n",
      "Epoch 9/99\n",
      "----------\n",
      "train loss: 0.5844 Acc: 0.7219\n",
      "val loss: 0.5188 Acc: 0.8380\n",
      "Epoch 10/99\n",
      "----------\n",
      "train loss: 0.5748 Acc: 0.7654\n",
      "val loss: 0.5160 Acc: 0.8436\n",
      "Epoch 11/99\n",
      "----------\n",
      "train loss: 0.5846 Acc: 0.7275\n",
      "val loss: 0.5138 Acc: 0.8436\n",
      "Epoch 12/99\n",
      "----------\n",
      "train loss: 0.5761 Acc: 0.7275\n",
      "val loss: 0.5111 Acc: 0.8436\n",
      "Epoch 13/99\n",
      "----------\n",
      "train loss: 0.5721 Acc: 0.7303\n",
      "val loss: 0.5087 Acc: 0.8380\n",
      "Epoch 14/99\n",
      "----------\n",
      "train loss: 0.5744 Acc: 0.7444\n",
      "val loss: 0.5085 Acc: 0.8380\n",
      "Epoch 15/99\n",
      "----------\n",
      "train loss: 0.5735 Acc: 0.7317\n",
      "val loss: 0.5084 Acc: 0.8380\n",
      "Epoch 16/99\n",
      "----------\n",
      "train loss: 0.5815 Acc: 0.7177\n",
      "val loss: 0.5082 Acc: 0.8380\n",
      "Epoch 17/99\n",
      "----------\n",
      "train loss: 0.5737 Acc: 0.7486\n",
      "val loss: 0.5081 Acc: 0.8380\n",
      "Epoch 18/99\n",
      "----------\n",
      "train loss: 0.5820 Acc: 0.7303\n",
      "val loss: 0.5079 Acc: 0.8380\n",
      "Epoch 19/99\n",
      "----------\n",
      "train loss: 0.5849 Acc: 0.7233\n",
      "val loss: 0.5078 Acc: 0.8380\n",
      "Epoch 20/99\n",
      "----------\n",
      "train loss: 0.5823 Acc: 0.7233\n",
      "val loss: 0.5076 Acc: 0.8380\n",
      "Epoch 21/99\n",
      "----------\n",
      "train loss: 0.5713 Acc: 0.7486\n",
      "val loss: 0.5076 Acc: 0.8380\n",
      "Epoch 22/99\n",
      "----------\n",
      "train loss: 0.5691 Acc: 0.7402\n",
      "val loss: 0.5076 Acc: 0.8380\n",
      "Epoch 23/99\n",
      "----------\n",
      "train loss: 0.5785 Acc: 0.7149\n",
      "val loss: 0.5076 Acc: 0.8380\n",
      "Epoch 24/99\n",
      "----------\n",
      "train loss: 0.5794 Acc: 0.7275\n",
      "val loss: 0.5076 Acc: 0.8380\n",
      "Epoch 25/99\n",
      "----------\n",
      "train loss: 0.5742 Acc: 0.7107\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 26/99\n",
      "----------\n",
      "train loss: 0.5801 Acc: 0.7163\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 27/99\n",
      "----------\n",
      "train loss: 0.5789 Acc: 0.7233\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 28/99\n",
      "----------\n",
      "train loss: 0.5731 Acc: 0.7514\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 29/99\n",
      "----------\n",
      "train loss: 0.5794 Acc: 0.7317\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 30/99\n",
      "----------\n",
      "train loss: 0.5655 Acc: 0.7683\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 31/99\n",
      "----------\n",
      "train loss: 0.5774 Acc: 0.7346\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 32/99\n",
      "----------\n",
      "train loss: 0.5773 Acc: 0.7346\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 33/99\n",
      "----------\n",
      "train loss: 0.5716 Acc: 0.7612\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 34/99\n",
      "----------\n",
      "train loss: 0.5842 Acc: 0.7331\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 35/99\n",
      "----------\n",
      "train loss: 0.5659 Acc: 0.7402\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 36/99\n",
      "----------\n",
      "train loss: 0.5938 Acc: 0.7051\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 37/99\n",
      "----------\n",
      "train loss: 0.5938 Acc: 0.7093\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 38/99\n",
      "----------\n",
      "train loss: 0.5732 Acc: 0.7374\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 39/99\n",
      "----------\n",
      "train loss: 0.5754 Acc: 0.7388\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 40/99\n",
      "----------\n",
      "train loss: 0.5816 Acc: 0.7416\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 41/99\n",
      "----------\n",
      "train loss: 0.5835 Acc: 0.7149\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 42/99\n",
      "----------\n",
      "train loss: 0.5890 Acc: 0.7022\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 43/99\n",
      "----------\n",
      "train loss: 0.5698 Acc: 0.7388\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 44/99\n",
      "----------\n",
      "train loss: 0.5883 Acc: 0.6938\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 45/99\n",
      "----------\n",
      "train loss: 0.5752 Acc: 0.7346\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 46/99\n",
      "----------\n",
      "train loss: 0.5810 Acc: 0.7149\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 47/99\n",
      "----------\n",
      "train loss: 0.5782 Acc: 0.7346\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 48/99\n",
      "----------\n",
      "train loss: 0.5773 Acc: 0.7346\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 49/99\n",
      "----------\n",
      "train loss: 0.5714 Acc: 0.7500\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 50/99\n",
      "----------\n",
      "train loss: 0.5776 Acc: 0.7191\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 51/99\n",
      "----------\n",
      "train loss: 0.5736 Acc: 0.7275\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 52/99\n",
      "----------\n",
      "train loss: 0.5776 Acc: 0.7360\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 53/99\n",
      "----------\n",
      "train loss: 0.5866 Acc: 0.7093\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 54/99\n",
      "----------\n",
      "train loss: 0.5821 Acc: 0.7275\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 55/99\n",
      "----------\n",
      "train loss: 0.5809 Acc: 0.7331\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 56/99\n",
      "----------\n",
      "train loss: 0.5720 Acc: 0.7486\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 57/99\n",
      "----------\n",
      "train loss: 0.5808 Acc: 0.7261\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 58/99\n",
      "----------\n",
      "train loss: 0.5676 Acc: 0.7542\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 59/99\n",
      "----------\n",
      "train loss: 0.5707 Acc: 0.7360\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 60/99\n",
      "----------\n",
      "train loss: 0.5844 Acc: 0.7163\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 61/99\n",
      "----------\n",
      "train loss: 0.5830 Acc: 0.7303\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 62/99\n",
      "----------\n",
      "train loss: 0.5807 Acc: 0.7458\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 63/99\n",
      "----------\n",
      "train loss: 0.5639 Acc: 0.7598\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 64/99\n",
      "----------\n",
      "train loss: 0.5853 Acc: 0.7121\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 65/99\n",
      "----------\n",
      "train loss: 0.5908 Acc: 0.6910\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 66/99\n",
      "----------\n",
      "train loss: 0.5734 Acc: 0.7430\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 67/99\n",
      "----------\n",
      "train loss: 0.5858 Acc: 0.7261\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 68/99\n",
      "----------\n",
      "train loss: 0.5711 Acc: 0.7598\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 69/99\n",
      "----------\n",
      "train loss: 0.5711 Acc: 0.7444\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 70/99\n",
      "----------\n",
      "train loss: 0.5783 Acc: 0.7219\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 71/99\n",
      "----------\n",
      "train loss: 0.5695 Acc: 0.7542\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 72/99\n",
      "----------\n",
      "train loss: 0.5836 Acc: 0.7177\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 73/99\n",
      "----------\n",
      "train loss: 0.5705 Acc: 0.7416\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 74/99\n",
      "----------\n",
      "train loss: 0.5814 Acc: 0.7163\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 75/99\n",
      "----------\n",
      "train loss: 0.5850 Acc: 0.7093\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 76/99\n",
      "----------\n",
      "train loss: 0.5761 Acc: 0.7402\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 77/99\n",
      "----------\n",
      "train loss: 0.5794 Acc: 0.7191\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 78/99\n",
      "----------\n",
      "train loss: 0.5815 Acc: 0.7233\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 79/99\n",
      "----------\n",
      "train loss: 0.5843 Acc: 0.7289\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 80/99\n",
      "----------\n",
      "train loss: 0.5882 Acc: 0.7275\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 81/99\n",
      "----------\n",
      "train loss: 0.5723 Acc: 0.7233\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 82/99\n",
      "----------\n",
      "train loss: 0.5725 Acc: 0.7191\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 83/99\n",
      "----------\n",
      "train loss: 0.5824 Acc: 0.7289\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 84/99\n",
      "----------\n",
      "train loss: 0.5791 Acc: 0.7233\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 85/99\n",
      "----------\n",
      "train loss: 0.5869 Acc: 0.7121\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 86/99\n",
      "----------\n",
      "train loss: 0.5749 Acc: 0.7317\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 87/99\n",
      "----------\n",
      "train loss: 0.5666 Acc: 0.7612\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 88/99\n",
      "----------\n",
      "train loss: 0.5765 Acc: 0.7374\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 89/99\n",
      "----------\n",
      "train loss: 0.5757 Acc: 0.7388\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 90/99\n",
      "----------\n",
      "train loss: 0.5784 Acc: 0.7317\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 91/99\n",
      "----------\n",
      "train loss: 0.5776 Acc: 0.7219\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 92/99\n",
      "----------\n",
      "train loss: 0.5850 Acc: 0.7205\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 93/99\n",
      "----------\n",
      "train loss: 0.5755 Acc: 0.7360\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 94/99\n",
      "----------\n",
      "train loss: 0.5764 Acc: 0.7388\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 95/99\n",
      "----------\n",
      "train loss: 0.5906 Acc: 0.6966\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 96/99\n",
      "----------\n",
      "train loss: 0.5695 Acc: 0.7514\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 97/99\n",
      "----------\n",
      "train loss: 0.5715 Acc: 0.7275\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 98/99\n",
      "----------\n",
      "train loss: 0.5675 Acc: 0.7247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.5075 Acc: 0.8380\n",
      "Epoch 99/99\n",
      "----------\n",
      "train loss: 0.5792 Acc: 0.7430\n",
      "val loss: 0.5075 Acc: 0.8380\n",
      "Trainning complete in 0m 7s\n",
      "Best val Acc: 0.843575\n"
     ]
    }
   ],
   "source": [
    "model=ClassifierModule()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=2e-2,momentum=0.9)\n",
    "exp_lr_scheduler=lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)\n",
    "\n",
    "model=train_model(model,criterion,optimizer,exp_lr_scheduler,dataset_sizes,num_epochs=100,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model and reload it\n",
    "torch.save(model.state_dict(),'pytorch_model.pth')\n",
    "reloaded_model=ClassifierModule()\n",
    "reloaded_model.load_state_dict(torch.load('pytorch_model.pth'))\n",
    "# predicts on  test dataset\n",
    "\n",
    "reloaded_model.eval()\n",
    "final_predicts=[]\n",
    "with torch.no_grad():\n",
    "    for samples in dataloaders['test']:\n",
    "        inputs=samples['X'].to(device)\n",
    "        outputs=reloaded_model(inputs)\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        for y_predict in list(preds.numpy()):\n",
    "            final_predicts.append(y_predict)\n",
    "final_predicts=np.array(final_predicts).reshape(-1,)\n",
    "predict_survived_pytorch=pd.Series(final_predicts,name='Survived')\n",
    "pytorch_result=pd.concat([IDtest,predict_survived_pytorch],axis=1)\n",
    "pytorch_result.to_csv('pytorch_result.csv',index=False)\n",
    "pytorch_result.head()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用skorch搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.7777       0.3596        0.7712  0.0215\n",
      "      2        0.7674       0.3596        0.7610  0.0090\n",
      "      3        0.7622       0.3596        0.7513  0.0145\n",
      "      4        0.7513       0.3596        0.7424  0.0145\n",
      "      5        0.7402       0.3596        0.7349  0.0125\n",
      "      6        0.7366       0.3596        0.7277  0.0110\n",
      "      7        0.7256       0.3596        0.7212  0.0110\n",
      "      8        0.7245       0.3596        0.7153  0.0100\n",
      "      9        0.7177       0.3596        0.7097  0.0125\n",
      "     10        0.7100       0.3596        0.7046  0.0135\n",
      "     11        0.7006       0.3820        0.7000  0.0105\n",
      "     12        0.7112       0.3933        0.6957  0.0135\n",
      "     13        0.6965       0.4157        0.6918  0.0120\n",
      "     14        0.6926       0.4382        0.6881  0.0110\n",
      "     15        0.6905       0.5618        0.6849  0.0125\n",
      "     16        0.6847       0.7640        0.6818  0.0110\n",
      "     17        0.6838       0.7753        0.6787  0.0140\n",
      "     18        0.6832       0.7753        0.6759  0.0120\n",
      "     19        0.6791       0.7528        0.6733  0.0115\n",
      "     20        0.6825       0.7079        0.6710  0.0130\n",
      "     21        0.6784       0.6742        0.6687  0.0105\n",
      "     22        0.6746       0.6629        0.6668  0.0150\n",
      "     23        0.6691       0.6517        0.6649  0.0140\n",
      "     24        0.6697       0.6517        0.6630  0.0140\n",
      "     25        0.6744       0.6517        0.6613  0.0160\n",
      "     26        0.6698       0.6404        0.6597  0.0160\n",
      "     27        0.6706       0.6404        0.6582  0.0140\n",
      "     28        0.6583       0.6404        0.6569  0.0130\n",
      "     29        0.6646       0.6404        0.6555  0.0165\n",
      "     30        0.6660       0.6404        0.6542  0.0160\n",
      "     31        0.6626       0.6404        0.6529  0.0100\n",
      "     32        0.6570       0.6404        0.6517  0.0140\n",
      "     33        0.6609       0.6404        0.6506  0.0140\n",
      "     34        0.6584       0.6404        0.6494  0.0090\n",
      "     35        0.6610       0.6404        0.6485  0.0105\n",
      "     36        0.6593       0.6404        0.6474  0.0115\n",
      "     37        0.6520       0.6404        0.6465  0.0130\n",
      "     38        0.6634       0.6404        0.6456  0.0125\n",
      "     39        0.6593       0.6404        0.6448  0.0135\n",
      "     40        0.6505       0.6404        0.6438  0.0140\n",
      "     41        0.6526       0.6404        0.6430  0.0095\n",
      "     42        0.6464       0.6404        0.6420  0.0135\n",
      "     43        0.6557       0.6404        0.6412  0.0160\n",
      "     44        0.6536       0.6404        0.6404  0.0135\n",
      "     45        0.6438       0.6404        0.6395  0.0115\n",
      "     46        0.6547       0.6404        0.6388  0.0185\n",
      "     47        0.6524       0.6404        0.6380  0.0125\n",
      "     48        0.6456       0.6404        0.6372  0.0130\n",
      "     49        0.6427       0.6404        0.6365  0.0160\n",
      "     50        0.6418       0.6404        0.6357  0.0110\n",
      "     51        0.6518       0.6404        0.6350  0.0120\n",
      "     52        0.6457       0.6404        0.6343  0.0120\n",
      "     53        0.6474       0.6404        0.6336  0.0150\n",
      "     54        0.6502       0.6404        0.6330  0.0140\n",
      "     55        0.6367       0.6404        0.6321  0.0110\n",
      "     56        0.6529       0.6404        0.6315  0.0145\n",
      "     57        0.6498       0.6404        0.6308  0.0130\n",
      "     58        0.6440       0.6404        0.6302  0.0125\n",
      "     59        0.6492       0.6404        0.6296  0.0125\n",
      "     60        0.6507       0.6404        0.6291  0.0100\n",
      "     61        0.6410       0.6404        0.6285  0.0130\n",
      "     62        0.6425       0.6404        0.6279  0.0120\n",
      "     63        0.6360       0.6404        0.6272  0.0125\n",
      "     64        0.6466       0.6404        0.6266  0.0130\n",
      "     65        0.6310       0.6404        0.6257  0.0130\n",
      "     66        0.6454       0.6404        0.6251  0.0135\n",
      "     67        0.6362       0.6404        0.6244  0.0105\n",
      "     68        0.6366       0.6404        0.6237  0.0130\n",
      "     69        0.6435       0.6404        0.6231  0.0140\n",
      "     70        0.6340       0.6404        0.6224  0.0115\n",
      "     71        0.6378       0.6404        0.6217  0.0105\n",
      "     72        0.6294       0.6404        0.6209  0.0120\n",
      "     73        0.6481       0.6404        0.6205  0.0120\n",
      "     74        0.6390       0.6404        0.6198  0.0125\n",
      "     75        0.6350       0.6404        0.6190  0.0135\n",
      "     76        0.6346       0.6404        0.6183  0.0125\n",
      "     77        0.6295       0.6404        0.6175  0.0125\n",
      "     78        0.6385       0.6404        0.6168  0.0135\n",
      "     79        0.6349       0.6404        0.6161  0.0125\n",
      "     80        0.6377       0.6404        0.6154  0.0105\n",
      "     81        0.6332       0.6404        0.6148  0.0085\n",
      "     82        0.6197       0.6404        0.6138  0.0095\n",
      "     83        0.6455       0.6404        0.6133  0.0100\n",
      "     84        0.6283       0.6404        0.6124  0.0100\n",
      "     85        0.6237       0.6404        0.6116  0.0110\n",
      "     86        0.6311       0.6404        0.6108  0.0080\n",
      "     87        0.6291       0.6404        0.6101  0.0100\n",
      "     88        0.6370       0.6404        0.6094  0.0130\n",
      "     89        0.6282       0.6404        0.6087  0.0130\n",
      "     90        0.6286       0.6404        0.6079  0.0115\n",
      "     91        0.6378       0.6404        0.6073  0.0115\n",
      "     92        0.6256       0.6404        0.6065  0.0115\n",
      "     93        0.6168       0.6404        0.6054  0.0095\n",
      "     94        0.6269       0.6404        0.6046  0.0100\n",
      "     95        0.6312       0.6404        0.6038  0.0105\n",
      "     96        0.6098       0.6517        0.6026  0.0130\n",
      "     97        0.6162       0.6517        0.6016  0.0135\n",
      "     98        0.6235       0.6629        0.6006  0.0145\n",
      "     99        0.6263       0.6629        0.5997  0.0100\n",
      "    100        0.6108       0.6742        0.5985  0.0105\n",
      "    101        0.6112       0.6742        0.5973  0.0130\n",
      "    102        0.6234       0.6742        0.5964  0.0155\n",
      "    103        0.6258       0.6742        0.5955  0.0140\n",
      "    104        0.6230       0.6742        0.5946  0.0120\n",
      "    105        0.6041       0.6742        0.5933  0.0329\n",
      "    106        0.6147       0.6742        0.5922  0.0100\n",
      "    107        0.5965       0.6742        0.5908  0.0165\n",
      "    108        0.6139       0.6742        0.5896  0.0130\n",
      "    109        0.6066       0.6742        0.5882  0.0090\n",
      "    110        0.6007       0.6854        0.5867  0.0105\n",
      "    111        0.6165       0.6854        0.5857  0.0105\n",
      "    112        0.6084       0.6854        0.5844  0.0125\n",
      "    113        0.6190       0.6966        0.5833  0.0120\n",
      "    114        0.6143       0.6966        0.5821  0.0105\n",
      "    115        0.6094       0.6966        0.5809  0.0120\n",
      "    116        0.5899       0.7079        0.5794  0.0105\n",
      "    117        0.6074       0.7079        0.5782  0.0110\n",
      "    118        0.6051       0.7079        0.5768  0.0135\n",
      "    119        0.6004       0.7079        0.5755  0.0125\n",
      "    120        0.6102       0.7079        0.5745  0.0105\n",
      "    121        0.6182       0.7079        0.5735  0.0120\n",
      "    122        0.5979       0.7079        0.5723  0.0105\n",
      "    123        0.6119       0.7079        0.5714  0.0115\n",
      "    124        0.5880       0.7079        0.5697  0.0120\n",
      "    125        0.6007       0.7079        0.5686  0.0115\n",
      "    126        0.6095       0.7191        0.5675  0.0140\n",
      "    127        0.5956       0.7191        0.5661  0.0120\n",
      "    128        0.5926       0.7191        0.5646  0.0125\n",
      "    129        0.5847       0.7191        0.5631  0.0105\n",
      "    130        0.5828       0.7303        0.5614  0.0125\n",
      "    131        0.5965       0.7416        0.5601  0.0165\n",
      "    132        0.5948       0.7416        0.5590  0.0105\n",
      "    133        0.5800       0.7416        0.5575  0.0110\n",
      "    134        0.5896       0.7416        0.5564  0.0135\n",
      "    135        0.5908       0.7416        0.5552  0.0105\n",
      "    136        0.5937       0.7416        0.5540  0.0120\n",
      "    137        0.5699       0.7528        0.5523  0.0140\n",
      "    138        0.5983       0.7528        0.5512  0.0085\n",
      "    139        0.5905       0.7528        0.5501  0.0130\n",
      "    140        0.5939       0.7528        0.5492  0.0090\n",
      "    141        0.5981       0.7640        0.5482  0.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    142        0.5899       0.7753        0.5471  0.0080\n",
      "    143        0.5901       0.7865        0.5462  0.0125\n",
      "    144        0.5750       0.7865        0.5447  0.0140\n",
      "    145        0.5878       0.7865        0.5435  0.0125\n",
      "    146        0.5909       0.7865        0.5425  0.0090\n",
      "    147        0.5714       0.7978        0.5410  0.0085\n",
      "    148        0.5854       0.7978        0.5400  0.0105\n",
      "    149        0.5767       0.7978        0.5390  0.0110\n",
      "    150        0.5808       0.7978        0.5379  0.0110\n",
      "    151        0.5680       0.7978        0.5365  0.0125\n",
      "    152        0.5918       0.7978        0.5357  0.0130\n",
      "    153        0.5818       0.8202        0.5346  0.0120\n",
      "    154        0.5749       0.8202        0.5333  0.0135\n",
      "    155        0.5736       0.8202        0.5323  0.0090\n",
      "    156        0.5680       0.8202        0.5312  0.0085\n",
      "    157        0.5671       0.8427        0.5298  0.0130\n",
      "    158        0.5642       0.8427        0.5285  0.0195\n",
      "    159        0.5744       0.8427        0.5272  0.0130\n",
      "    160        0.5620       0.8427        0.5262  0.0100\n",
      "    161        0.5665       0.8427        0.5251  0.0130\n",
      "    162        0.5675       0.8427        0.5241  0.0140\n",
      "    163        0.5708       0.8427        0.5233  0.0120\n",
      "    164        0.5574       0.8427        0.5216  0.0140\n",
      "    165        0.5628       0.8427        0.5205  0.0130\n",
      "    166        0.5825       0.8427        0.5197  0.0165\n",
      "    167        0.5668       0.8427        0.5188  0.0120\n",
      "    168        0.5575       0.8427        0.5178  0.0125\n",
      "    169        0.5608       0.8427        0.5165  0.0105\n",
      "    170        0.5633       0.8427        0.5156  0.0100\n",
      "    171        0.5552       0.8427        0.5146  0.0090\n",
      "    172        0.5354       0.8427        0.5130  0.0135\n",
      "    173        0.5615       0.8427        0.5120  0.0115\n",
      "    174        0.5608       0.8427        0.5110  0.0100\n",
      "    175        0.5797       0.8427        0.5105  0.0120\n",
      "    176        0.5583       0.8427        0.5095  0.0135\n",
      "    177        0.5747       0.8427        0.5091  0.0165\n",
      "    178        0.5480       0.8427        0.5076  0.0090\n",
      "    179        0.5619       0.8427        0.5068  0.0125\n",
      "    180        0.5591       0.8427        0.5056  0.0120\n",
      "    181        0.5454       0.8427        0.5044  0.0110\n",
      "    182        0.5554       0.8652        0.5035  0.0140\n",
      "    183        0.5608       0.8652        0.5027  0.0150\n",
      "    184        0.5607       0.8652        0.5019  0.0155\n",
      "    185        0.5748       0.8539        0.5012  0.0130\n",
      "    186        0.5401       0.8764        0.5003  0.0110\n",
      "    187        0.5500       0.8764        0.4993  0.0120\n",
      "    188        0.5449       0.8652        0.4983  0.0130\n",
      "    189        0.5554       0.8652        0.4974  0.0125\n",
      "    190        0.5586       0.8652        0.4966  0.0100\n",
      "    191        0.5599       0.8539        0.4957  0.0095\n",
      "    192        0.5618       0.8652        0.4950  0.0095\n",
      "    193        0.5444       0.8652        0.4940  0.0110\n",
      "    194        0.5450       0.8652        0.4931  0.0120\n",
      "    195        0.5322       0.8652        0.4920  0.0110\n",
      "    196        0.5343       0.8652        0.4908  0.0135\n",
      "    197        0.5354       0.8652        0.4898  0.0100\n",
      "    198        0.5284       0.8652        0.4889  0.0100\n",
      "    199        0.5406       0.8652        0.4882  0.0135\n",
      "    200        0.5483       0.8539        0.4879  0.0085\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.7334       0.4333        0.7205  0.0095\n",
      "      2        0.7244       0.4333        0.7173  0.0115\n",
      "      3        0.7214       0.4333        0.7138  0.0145\n",
      "      4        0.7226       0.4333        0.7107  0.0100\n",
      "      5        0.7093       0.4333        0.7078  0.0125\n",
      "      6        0.7137       0.4333        0.7056  0.0140\n",
      "      7        0.7037       0.4333        0.7031  0.0110\n",
      "      8        0.7080       0.4333        0.7009  0.0095\n",
      "      9        0.6986       0.4333        0.6988  0.0120\n",
      "     10        0.6959       0.4444        0.6965  0.0120\n",
      "     11        0.6883       0.5444        0.6944  0.0115\n",
      "     12        0.6920       0.5000        0.6927  0.0145\n",
      "     13        0.6904       0.5333        0.6907  0.0095\n",
      "     14        0.6946       0.5444        0.6895  0.0125\n",
      "     15        0.6902       0.5222        0.6885  0.0115\n",
      "     16        0.6841       0.5222        0.6873  0.0120\n",
      "     17        0.6853       0.5444        0.6861  0.0135\n",
      "     18        0.6886       0.5556        0.6850  0.0120\n",
      "     19        0.6872       0.5889        0.6840  0.0110\n",
      "     20        0.6734       0.5889        0.6830  0.0115\n",
      "     21        0.6852       0.6111        0.6820  0.0130\n",
      "     22        0.6838       0.6222        0.6812  0.0180\n",
      "     23        0.6741       0.6222        0.6803  0.0140\n",
      "     24        0.6773       0.6222        0.6793  0.0100\n",
      "     25        0.6770       0.6222        0.6784  0.0150\n",
      "     26        0.6737       0.6111        0.6777  0.0110\n",
      "     27        0.6626       0.5889        0.6770  0.0105\n",
      "     28        0.6608       0.5778        0.6762  0.0100\n",
      "     29        0.6728       0.5778        0.6756  0.0120\n",
      "     30        0.6822       0.5667        0.6751  0.0090\n",
      "     31        0.6637       0.5667        0.6744  0.0095\n",
      "     32        0.6517       0.5667        0.6739  0.0140\n",
      "     33        0.6743       0.5667        0.6736  0.0150\n",
      "     34        0.6707       0.5667        0.6732  0.0115\n",
      "     35        0.6723       0.5667        0.6729  0.0085\n",
      "     36        0.6619       0.5667        0.6723  0.0110\n",
      "     37        0.6731       0.5667        0.6721  0.0120\n",
      "     38        0.6619       0.5667        0.6717  0.0110\n",
      "     39        0.6550       0.5667        0.6713  0.0110\n",
      "     40        0.6690       0.5667        0.6710  0.0110\n",
      "     41        0.6502       0.5667        0.6705  0.0115\n",
      "     42        0.6461       0.5667        0.6700  0.0125\n",
      "     43        0.6548       0.5667        0.6696  0.0130\n",
      "     44        0.6616       0.5667        0.6693  0.0085\n",
      "     45        0.6592       0.5667        0.6691  0.0080\n",
      "     46        0.6567       0.5667        0.6687  0.0120\n",
      "     47        0.6580       0.5667        0.6686  0.0125\n",
      "     48        0.6522       0.5667        0.6682  0.0125\n",
      "     49        0.6530       0.5667        0.6678  0.0110\n",
      "     50        0.6569       0.5667        0.6674  0.0085\n",
      "     51        0.6574       0.5667        0.6672  0.0115\n",
      "     52        0.6572       0.5667        0.6668  0.0100\n",
      "     53        0.6533       0.5667        0.6663  0.0090\n",
      "     54        0.6564       0.5667        0.6661  0.0140\n",
      "     55        0.6549       0.5667        0.6657  0.0100\n",
      "     56        0.6487       0.5667        0.6654  0.0115\n",
      "     57        0.6542       0.5667        0.6650  0.0100\n",
      "     58        0.6549       0.5667        0.6648  0.0110\n",
      "     59        0.6502       0.5667        0.6645  0.0130\n",
      "     60        0.6459       0.5667        0.6642  0.0085\n",
      "     61        0.6382       0.5667        0.6637  0.0105\n",
      "     62        0.6428       0.5667        0.6633  0.0105\n",
      "     63        0.6458       0.5667        0.6629  0.0110\n",
      "     64        0.6381       0.5667        0.6626  0.0105\n",
      "     65        0.6516       0.5667        0.6623  0.0105\n",
      "     66        0.6496       0.5667        0.6620  0.0115\n",
      "     67        0.6533       0.5667        0.6617  0.0100\n",
      "     68        0.6483       0.5667        0.6614  0.0110\n",
      "     69        0.6382       0.5667        0.6610  0.0085\n",
      "     70        0.6423       0.5667        0.6606  0.0075\n",
      "     71        0.6493       0.5667        0.6602  0.0095\n",
      "     72        0.6529       0.5667        0.6599  0.0120\n",
      "     73        0.6484       0.5667        0.6595  0.0135\n",
      "     74        0.6506       0.5667        0.6593  0.0105\n",
      "     75        0.6433       0.5667        0.6587  0.0105\n",
      "     76        0.6497       0.5667        0.6585  0.0120\n",
      "     77        0.6447       0.5667        0.6581  0.0080\n",
      "     78        0.6424       0.5667        0.6578  0.0115\n",
      "     79        0.6337       0.5667        0.6571  0.0120\n",
      "     80        0.6354       0.5667        0.6566  0.0125\n",
      "     81        0.6393       0.5667        0.6561  0.0145\n",
      "     82        0.6396       0.5667        0.6558  0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83        0.6376       0.5667        0.6552  0.0130\n",
      "     84        0.6400       0.5667        0.6545  0.0100\n",
      "     85        0.6378       0.5667        0.6539  0.0100\n",
      "     86        0.6429       0.5667        0.6534  0.0155\n",
      "     87        0.6344       0.5667        0.6528  0.0165\n",
      "     88        0.6241       0.5667        0.6521  0.0125\n",
      "     89        0.6505       0.5667        0.6517  0.0155\n",
      "     90        0.6300       0.5667        0.6512  0.0120\n",
      "     91        0.6369       0.5667        0.6509  0.0120\n",
      "     92        0.6375       0.5667        0.6504  0.0125\n",
      "     93        0.6346       0.5667        0.6499  0.0115\n",
      "     94        0.6304       0.5667        0.6492  0.0125\n",
      "     95        0.6393       0.5667        0.6488  0.0085\n",
      "     96        0.6301       0.5667        0.6483  0.0115\n",
      "     97        0.6365       0.5667        0.6477  0.0120\n",
      "     98        0.6270       0.5667        0.6470  0.0090\n",
      "     99        0.6280       0.5667        0.6463  0.0100\n",
      "    100        0.6315       0.5667        0.6457  0.0105\n",
      "    101        0.6210       0.5667        0.6448  0.0110\n",
      "    102        0.6358       0.5667        0.6443  0.0125\n",
      "    103        0.6214       0.5667        0.6439  0.0100\n",
      "    104        0.6219       0.5667        0.6434  0.0100\n",
      "    105        0.6298       0.5667        0.6428  0.0090\n",
      "    106        0.6313       0.5667        0.6422  0.0110\n",
      "    107        0.6348       0.5667        0.6419  0.0095\n",
      "    108        0.6256       0.5667        0.6411  0.0105\n",
      "    109        0.6332       0.5667        0.6406  0.0130\n",
      "    110        0.6264       0.5667        0.6399  0.0170\n",
      "    111        0.6162       0.5778        0.6393  0.0105\n",
      "    112        0.6255       0.5778        0.6387  0.0130\n",
      "    113        0.6318       0.5778        0.6381  0.0105\n",
      "    114        0.6450       0.5778        0.6379  0.0110\n",
      "    115        0.6180       0.5778        0.6373  0.0135\n",
      "    116        0.6258       0.6000        0.6364  0.0140\n",
      "    117        0.6162       0.6000        0.6357  0.0110\n",
      "    118        0.6177       0.6000        0.6350  0.0145\n",
      "    119        0.6340       0.6000        0.6348  0.0120\n",
      "    120        0.6232       0.6000        0.6341  0.0095\n",
      "    121        0.6155       0.6000        0.6333  0.0105\n",
      "    122        0.6072       0.6111        0.6324  0.0140\n",
      "    123        0.6234       0.6111        0.6317  0.0120\n",
      "    124        0.6380       0.6111        0.6316  0.0130\n",
      "    125        0.6296       0.6111        0.6309  0.0100\n",
      "    126        0.6022       0.6111        0.6296  0.0090\n",
      "    127        0.6307       0.6111        0.6298  0.0120\n",
      "    128        0.6321       0.6111        0.6293  0.0115\n",
      "    129        0.6139       0.6111        0.6283  0.0115\n",
      "    130        0.6073       0.6111        0.6275  0.0115\n",
      "    131        0.6150       0.6111        0.6268  0.0120\n",
      "    132        0.6322       0.6111        0.6265  0.0135\n",
      "    133        0.6260       0.6111        0.6258  0.0105\n",
      "    134        0.6197       0.6111        0.6251  0.0115\n",
      "    135        0.6063       0.6111        0.6240  0.0145\n",
      "    136        0.6097       0.6111        0.6232  0.0125\n",
      "    137        0.6075       0.6111        0.6226  0.0140\n",
      "    138        0.6078       0.6111        0.6220  0.0100\n",
      "    139        0.6102       0.6111        0.6212  0.0095\n",
      "    140        0.6384       0.6111        0.6211  0.0115\n",
      "    141        0.6159       0.6111        0.6203  0.0100\n",
      "    142        0.6211       0.6111        0.6196  0.0100\n",
      "    143        0.6191       0.6222        0.6189  0.0125\n",
      "    144        0.6199       0.6111        0.6184  0.0110\n",
      "    145        0.6083       0.6111        0.6179  0.0125\n",
      "    146        0.6173       0.6222        0.6172  0.0125\n",
      "    147        0.6293       0.6222        0.6168  0.0105\n",
      "    148        0.6153       0.6333        0.6162  0.0090\n",
      "    149        0.6135       0.6333        0.6152  0.0110\n",
      "    150        0.6055       0.6333        0.6142  0.0130\n",
      "    151        0.6201       0.6333        0.6134  0.0115\n",
      "    152        0.6166       0.6333        0.6130  0.0085\n",
      "    153        0.6091       0.6333        0.6125  0.0110\n",
      "    154        0.6110       0.6333        0.6117  0.0145\n",
      "    155        0.5965       0.6333        0.6105  0.0110\n",
      "    156        0.6104       0.6333        0.6096  0.0145\n",
      "    157        0.5976       0.6333        0.6090  0.0125\n",
      "    158        0.5949       0.6444        0.6080  0.0120\n",
      "    159        0.6311       0.6556        0.6077  0.0140\n",
      "    160        0.6044       0.6556        0.6069  0.0125\n",
      "    161        0.6180       0.6556        0.6066  0.0125\n",
      "    162        0.5961       0.6556        0.6060  0.0130\n",
      "    163        0.5988       0.6556        0.6051  0.0135\n",
      "    164        0.6074       0.6556        0.6044  0.0135\n",
      "    165        0.6034       0.6556        0.6035  0.0160\n",
      "    166        0.6075       0.6667        0.6024  0.0125\n",
      "    167        0.5896       0.6667        0.6012  0.0125\n",
      "    168        0.5997       0.6667        0.6003  0.0155\n",
      "    169        0.6109       0.6667        0.6000  0.0120\n",
      "    170        0.5907       0.6667        0.5986  0.0120\n",
      "    171        0.6163       0.6667        0.5980  0.0135\n",
      "    172        0.6217       0.6667        0.5977  0.0100\n",
      "    173        0.5939       0.6667        0.5968  0.0110\n",
      "    174        0.6095       0.6667        0.5962  0.0125\n",
      "    175        0.5854       0.6667        0.5948  0.0115\n",
      "    176        0.6049       0.6667        0.5940  0.0085\n",
      "    177        0.5915       0.6667        0.5930  0.0110\n",
      "    178        0.5900       0.6667        0.5918  0.0095\n",
      "    179        0.5906       0.6667        0.5907  0.0095\n",
      "    180        0.6147       0.6667        0.5905  0.0125\n",
      "    181        0.5929       0.6667        0.5898  0.0090\n",
      "    182        0.6055       0.6667        0.5896  0.0100\n",
      "    183        0.6015       0.6667        0.5889  0.0110\n",
      "    184        0.6225       0.6667        0.5895  0.0115\n",
      "    185        0.6199       0.6667        0.5891  0.0095\n",
      "    186        0.6305       0.6667        0.5895  0.0120\n",
      "    187        0.5994       0.6667        0.5890  0.0115\n",
      "    188        0.6164       0.6667        0.5880  0.0115\n",
      "    189        0.5832       0.6778        0.5870  0.0095\n",
      "    190        0.5804       0.6778        0.5859  0.0150\n",
      "    191        0.5904       0.6778        0.5845  0.0120\n",
      "    192        0.5871       0.6889        0.5831  0.0100\n",
      "    193        0.6091       0.7000        0.5826  0.0130\n",
      "    194        0.5892       0.6889        0.5822  0.0095\n",
      "    195        0.6322       0.6889        0.5826  0.0130\n",
      "    196        0.5998       0.7000        0.5815  0.0085\n",
      "    197        0.5910       0.7000        0.5803  0.0110\n",
      "    198        0.5814       0.7000        0.5794  0.0145\n",
      "    199        0.5897       0.7000        0.5783  0.0120\n",
      "    200        0.5765       0.7000        0.5774  0.0120\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6797       0.6704        0.6408  0.0250\n",
      "      2        0.6756       0.6704        0.6409  0.0250\n",
      "      3        0.6767       0.6704        0.6409  0.0250\n",
      "      4        0.6730       0.6704        0.6407  0.0230\n",
      "      5        0.6738       0.6704        0.6406  0.0225\n",
      "      6        0.6721       0.6704        0.6407  0.0205\n",
      "      7        0.6740       0.6704        0.6406  0.0180\n",
      "      8        0.6749       0.6704        0.6404  0.0235\n",
      "      9        0.6732       0.6704        0.6404  0.0180\n",
      "     10        0.6738       0.6704        0.6404  0.0190\n",
      "     11        0.6719       0.6704        0.6404  0.0190\n",
      "     12        0.6723       0.6704        0.6403  0.0230\n",
      "     13        0.6714       0.6704        0.6401  0.0210\n",
      "     14        0.6724       0.6704        0.6399  0.0359\n",
      "     15        0.6690       0.6704        0.6397  0.0210\n",
      "     16        0.6714       0.6704        0.6396  0.0200\n",
      "     17        0.6741       0.6704        0.6396  0.0195\n",
      "     18        0.6699       0.6704        0.6394  0.0205\n",
      "     19        0.6691       0.6704        0.6392  0.0225\n",
      "     20        0.6681       0.6704        0.6391  0.0274\n",
      "     21        0.6681       0.6704        0.6388  0.0245\n",
      "     22        0.6689       0.6704        0.6387  0.0255\n",
      "     23        0.6673       0.6704        0.6386  0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24        0.6657       0.6704        0.6384  0.0195\n",
      "     25        0.6674       0.6704        0.6382  0.0215\n",
      "     26        0.6686       0.6704        0.6380  0.0190\n",
      "     27        0.6668       0.6704        0.6378  0.0175\n",
      "     28        0.6663       0.6704        0.6376  0.0205\n",
      "     29        0.6696       0.6704        0.6374  0.0185\n",
      "     30        0.6684       0.6704        0.6372  0.0190\n",
      "     31        0.6633       0.6704        0.6370  0.0180\n",
      "     32        0.6657       0.6704        0.6366  0.0200\n",
      "     33        0.6664       0.6704        0.6363  0.0190\n",
      "     34        0.6666       0.6704        0.6360  0.0165\n",
      "     35        0.6640       0.6704        0.6358  0.0195\n",
      "     36        0.6652       0.6704        0.6354  0.0165\n",
      "     37        0.6649       0.6704        0.6352  0.0210\n",
      "     38        0.6633       0.6704        0.6350  0.0190\n",
      "     39        0.6649       0.6704        0.6347  0.0215\n",
      "     40        0.6617       0.6704        0.6345  0.0195\n",
      "     41        0.6622       0.6704        0.6342  0.0200\n",
      "     42        0.6630       0.6704        0.6339  0.0175\n",
      "     43        0.6611       0.6704        0.6335  0.0190\n",
      "     44        0.6583       0.6704        0.6332  0.0205\n",
      "     45        0.6587       0.6704        0.6328  0.0190\n",
      "     46        0.6587       0.6704        0.6325  0.0299\n",
      "     47        0.6591       0.6704        0.6322  0.0225\n",
      "     48        0.6600       0.6704        0.6319  0.0215\n",
      "     49        0.6627       0.6704        0.6317  0.0235\n",
      "     50        0.6584       0.6704        0.6312  0.0215\n",
      "     51        0.6616       0.6704        0.6309  0.0205\n",
      "     52        0.6609       0.6704        0.6305  0.0190\n",
      "     53        0.6569       0.6704        0.6301  0.0200\n",
      "     54        0.6616       0.6704        0.6298  0.0210\n",
      "     55        0.6565       0.6704        0.6294  0.0185\n",
      "     56        0.6633       0.6704        0.6291  0.0195\n",
      "     57        0.6585       0.6704        0.6289  0.0205\n",
      "     58        0.6602       0.6704        0.6284  0.0220\n",
      "     59        0.6586       0.6704        0.6281  0.0245\n",
      "     60        0.6535       0.6704        0.6276  0.0210\n",
      "     61        0.6567       0.6704        0.6273  0.0225\n",
      "     62        0.6490       0.6704        0.6268  0.0215\n",
      "     63        0.6551       0.6704        0.6263  0.0245\n",
      "     64        0.6463       0.6704        0.6257  0.0225\n",
      "     65        0.6490       0.6704        0.6250  0.0215\n",
      "     66        0.6593       0.6704        0.6246  0.0210\n",
      "     67        0.6475       0.6704        0.6238  0.0185\n",
      "     68        0.6521       0.6704        0.6233  0.0210\n",
      "     69        0.6505       0.6704        0.6227  0.0210\n",
      "     70        0.6447       0.6704        0.6220  0.0220\n",
      "     71        0.6462       0.6704        0.6214  0.0220\n",
      "     72        0.6460       0.6704        0.6207  0.0185\n",
      "     73        0.6376       0.6704        0.6198  0.0170\n",
      "     74        0.6451       0.6704        0.6191  0.0210\n",
      "     75        0.6486       0.6704        0.6185  0.0190\n",
      "     76        0.6507       0.6704        0.6179  0.0205\n",
      "     77        0.6473       0.6704        0.6173  0.0210\n",
      "     78        0.6388       0.6704        0.6165  0.0255\n",
      "     79        0.6459       0.6704        0.6158  0.0284\n",
      "     80        0.6445       0.6704        0.6151  0.0250\n",
      "     81        0.6417       0.6704        0.6142  0.0259\n",
      "     82        0.6473       0.6704        0.6137  0.0274\n",
      "     83        0.6453       0.6704        0.6129  0.0240\n",
      "     84        0.6410       0.6704        0.6120  0.0220\n",
      "     85        0.6489       0.6704        0.6114  0.0235\n",
      "     86        0.6324       0.6704        0.6105  0.0225\n",
      "     87        0.6443       0.6704        0.6097  0.0245\n",
      "     88        0.6391       0.6704        0.6088  0.0270\n",
      "     89        0.6449       0.6704        0.6081  0.0235\n",
      "     90        0.6337       0.6704        0.6071  0.0210\n",
      "     91        0.6353       0.6816        0.6062  0.0215\n",
      "     92        0.6363       0.6816        0.6053  0.0210\n",
      "     93        0.6288       0.6816        0.6043  0.0190\n",
      "     94        0.6298       0.6816        0.6033  0.0210\n",
      "     95        0.6406       0.6816        0.6025  0.0210\n",
      "     96        0.6304       0.6872        0.6016  0.0210\n",
      "     97        0.6346       0.6872        0.6008  0.0215\n",
      "     98        0.6167       0.6983        0.5995  0.0215\n",
      "     99        0.6285       0.7039        0.5986  0.0215\n",
      "    100        0.6310       0.7039        0.5977  0.0240\n",
      "    101        0.6277       0.7039        0.5970  0.0185\n",
      "    102        0.6279       0.6983        0.5959  0.0245\n",
      "    103        0.6211       0.7095        0.5948  0.0185\n",
      "    104        0.6243       0.7095        0.5938  0.0220\n",
      "    105        0.6185       0.7095        0.5927  0.0215\n",
      "    106        0.6342       0.7095        0.5920  0.0220\n",
      "    107        0.6221       0.7095        0.5909  0.0200\n",
      "    108        0.6178       0.7095        0.5895  0.0255\n",
      "    109        0.6055       0.7095        0.5880  0.0190\n",
      "    110        0.6225       0.7095        0.5869  0.0205\n",
      "    111        0.6248       0.7095        0.5859  0.0210\n",
      "    112        0.6220       0.6983        0.5849  0.0185\n",
      "    113        0.6148       0.7039        0.5840  0.0220\n",
      "    114        0.6201       0.7039        0.5829  0.0210\n",
      "    115        0.6180       0.7039        0.5818  0.0245\n",
      "    116        0.6111       0.7039        0.5805  0.0225\n",
      "    117        0.6315       0.7039        0.5801  0.0215\n",
      "    118        0.6042       0.7095        0.5786  0.0264\n",
      "    119        0.6122       0.7095        0.5778  0.0225\n",
      "    120        0.6130       0.7095        0.5767  0.0215\n",
      "    121        0.6069       0.7095        0.5756  0.0195\n",
      "    122        0.6081       0.7095        0.5745  0.0210\n",
      "    123        0.6076       0.7095        0.5733  0.0235\n",
      "    124        0.6158       0.7151        0.5726  0.0185\n",
      "    125        0.5873       0.7263        0.5708  0.0220\n",
      "    126        0.5998       0.7263        0.5695  0.0220\n",
      "    127        0.5960       0.7263        0.5678  0.0200\n",
      "    128        0.6118       0.7263        0.5670  0.0200\n",
      "    129        0.5879       0.7318        0.5655  0.0210\n",
      "    130        0.5948       0.7318        0.5641  0.0185\n",
      "    131        0.6111       0.7318        0.5634  0.0190\n",
      "    132        0.5946       0.7151        0.5623  0.0200\n",
      "    133        0.6092       0.7263        0.5613  0.0170\n",
      "    134        0.5923       0.7151        0.5601  0.0274\n",
      "    135        0.5979       0.7207        0.5591  0.0215\n",
      "    136        0.5935       0.7151        0.5578  0.0225\n",
      "    137        0.5919       0.7207        0.5567  0.0269\n",
      "    138        0.5824       0.7318        0.5552  0.0244\n",
      "    139        0.5908       0.7318        0.5538  0.0304\n",
      "    140        0.5623       0.7318        0.5520  0.0245\n",
      "    141        0.5938       0.7430        0.5512  0.0225\n",
      "    142        0.5748       0.7430        0.5498  0.0220\n",
      "    143        0.5964       0.7430        0.5492  0.0215\n",
      "    144        0.5875       0.7430        0.5481  0.0195\n",
      "    145        0.5851       0.7430        0.5469  0.0230\n",
      "    146        0.5734       0.7430        0.5457  0.0210\n",
      "    147        0.5918       0.7430        0.5449  0.0235\n",
      "    148        0.5854       0.7430        0.5438  0.0205\n",
      "    149        0.5816       0.7486        0.5427  0.0215\n",
      "    150        0.5823       0.7542        0.5415  0.0220\n",
      "    151        0.5887       0.7542        0.5404  0.0225\n",
      "    152        0.5773       0.7542        0.5391  0.0195\n",
      "    153        0.5831       0.7654        0.5384  0.0215\n",
      "    154        0.5721       0.7598        0.5369  0.0245\n",
      "    155        0.5697       0.7598        0.5355  0.0210\n",
      "    156        0.5666       0.7598        0.5342  0.0215\n",
      "    157        0.5803       0.7598        0.5333  0.0220\n",
      "    158        0.5810       0.7654        0.5320  0.0220\n",
      "    159        0.5604       0.7542        0.5305  0.0240\n",
      "    160        0.5724       0.7542        0.5294  0.0260\n",
      "    161        0.5588       0.7542        0.5280  0.0230\n",
      "    162        0.5493       0.7542        0.5264  0.0264\n",
      "    163        0.5485       0.7598        0.5248  0.0220\n",
      "    164        0.5602       0.7598        0.5233  0.0264\n",
      "    165        0.5526       0.7598        0.5224  0.0180\n",
      "    166        0.5549       0.7709        0.5214  0.0205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    167        0.5753       0.7709        0.5211  0.0245\n",
      "    168        0.5539       0.7709        0.5198  0.0250\n",
      "    169        0.5768       0.7709        0.5192  0.0220\n",
      "    170        0.5715       0.7709        0.5185  0.0205\n",
      "    171        0.5520       0.7709        0.5176  0.0200\n",
      "    172        0.5551       0.7709        0.5167  0.0220\n",
      "    173        0.5649       0.7709        0.5157  0.0205\n",
      "    174        0.5605       0.7709        0.5146  0.0195\n",
      "    175        0.5507       0.7709        0.5136  0.0185\n",
      "    176        0.5541       0.7654        0.5128  0.0200\n",
      "    177        0.5628       0.7709        0.5121  0.0210\n",
      "    178        0.5452       0.7654        0.5107  0.0264\n",
      "    179        0.5745       0.7709        0.5103  0.0260\n",
      "    180        0.5342       0.7654        0.5089  0.0245\n",
      "    181        0.5584       0.7654        0.5083  0.0230\n",
      "    182        0.5395       0.7654        0.5068  0.0225\n",
      "    183        0.5463       0.7654        0.5055  0.0230\n",
      "    184        0.5305       0.7654        0.5037  0.0240\n",
      "    185        0.5616       0.7654        0.5033  0.0225\n",
      "    186        0.5612       0.7709        0.5029  0.0200\n",
      "    187        0.5555       0.7654        0.5025  0.0205\n",
      "    188        0.5419       0.7654        0.5012  0.0200\n",
      "    189        0.5553       0.7654        0.5005  0.0225\n",
      "    190        0.5373       0.7654        0.4995  0.0225\n",
      "    191        0.5584       0.7654        0.4991  0.0210\n",
      "    192        0.5443       0.7542        0.4986  0.0200\n",
      "    193        0.5420       0.7654        0.4979  0.0240\n",
      "    194        0.5388       0.7654        0.4970  0.0205\n",
      "    195        0.5372       0.7654        0.4962  0.0190\n",
      "    196        0.5277       0.7542        0.4951  0.0170\n",
      "    197        0.5351       0.7654        0.4940  0.0200\n",
      "    198        0.5426       0.7654        0.4929  0.0200\n",
      "    199        0.5425       0.7654        0.4917  0.0205\n",
      "    200        0.5370       0.7709        0.4915  0.0200\n",
      "0.7845117845117845 {'lr': 0.02, 'max_epochs': 200, 'module__num_units': 20}\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.7113       0.6704        0.6715  0.0190\n",
      "      2        0.7007       0.6704        0.6704  0.0264\n",
      "      3        0.7068       0.6704        0.6698  0.0215\n",
      "      4        0.6992       0.6704        0.6688  0.0215\n",
      "      5        0.6931       0.6704        0.6677  0.0255\n",
      "      6        0.6947       0.6704        0.6669  0.0294\n",
      "      7        0.6887       0.6704        0.6659  0.0225\n",
      "      8        0.6906       0.6704        0.6650  0.0329\n",
      "      9        0.6995       0.6704        0.6644  0.0250\n",
      "     10        0.6948       0.6704        0.6638  0.0210\n",
      "     11        0.6950       0.6704        0.6633  0.0255\n",
      "     12        0.6940       0.6704        0.6627  0.0205\n",
      "     13        0.6881       0.6704        0.6618  0.0255\n",
      "     14        0.6950       0.6704        0.6615  0.0190\n",
      "     15        0.6848       0.6704        0.6605  0.0205\n",
      "     16        0.6855       0.6704        0.6598  0.0309\n",
      "     17        0.6814       0.6704        0.6589  0.0250\n",
      "     18        0.6820       0.6704        0.6581  0.0230\n",
      "     19        0.6840       0.6704        0.6574  0.0240\n",
      "     20        0.6860       0.6704        0.6570  0.0240\n",
      "     21        0.6731       0.6704        0.6562  0.0220\n",
      "     22        0.6799       0.6704        0.6555  0.0240\n",
      "     23        0.6845       0.6704        0.6551  0.0235\n",
      "     24        0.6807       0.6704        0.6545  0.0190\n",
      "     25        0.6816       0.6704        0.6540  0.0250\n",
      "     26        0.6797       0.6704        0.6537  0.0244\n",
      "     27        0.6789       0.6704        0.6532  0.0200\n",
      "     28        0.6705       0.6704        0.6524  0.0240\n",
      "     29        0.6801       0.6704        0.6520  0.0200\n",
      "     30        0.6747       0.6704        0.6514  0.0210\n",
      "     31        0.6758       0.6704        0.6511  0.0235\n",
      "     32        0.6747       0.6704        0.6506  0.0210\n",
      "     33        0.6727       0.6704        0.6501  0.0205\n",
      "     34        0.6764       0.6704        0.6498  0.0274\n",
      "     35        0.6734       0.6704        0.6493  0.0260\n",
      "     36        0.6753       0.6704        0.6491  0.0250\n",
      "     37        0.6682       0.6704        0.6485  0.0289\n",
      "     38        0.6709       0.6704        0.6481  0.0230\n",
      "     39        0.6738       0.6704        0.6478  0.0215\n",
      "     40        0.6759       0.6704        0.6476  0.0190\n",
      "     41        0.6738       0.6704        0.6474  0.0225\n",
      "     42        0.6725       0.6704        0.6471  0.0215\n",
      "     43        0.6766       0.6704        0.6469  0.0190\n",
      "     44        0.6666       0.6704        0.6463  0.0225\n",
      "     45        0.6708       0.6704        0.6461  0.0225\n",
      "     46        0.6731       0.6704        0.6457  0.0215\n",
      "     47        0.6673       0.6704        0.6454  0.0225\n",
      "     48        0.6677       0.6704        0.6450  0.0200\n",
      "     49        0.6709       0.6704        0.6447  0.0279\n",
      "     50        0.6703       0.6704        0.6445  0.0269\n",
      "     51        0.6677       0.6704        0.6442  0.0274\n",
      "     52        0.6673       0.6704        0.6439  0.0225\n",
      "     53        0.6687       0.6704        0.6436  0.0245\n",
      "     54        0.6686       0.6704        0.6433  0.0220\n",
      "     55        0.6660       0.6704        0.6429  0.0185\n",
      "     56        0.6657       0.6704        0.6426  0.0230\n",
      "     57        0.6667       0.6704        0.6423  0.0200\n",
      "     58        0.6660       0.6704        0.6420  0.0180\n",
      "     59        0.6615       0.6704        0.6415  0.0225\n",
      "     60        0.6662       0.6704        0.6412  0.0210\n",
      "     61        0.6657       0.6704        0.6409  0.0215\n",
      "     62        0.6681       0.6704        0.6407  0.0185\n",
      "     63        0.6678       0.6704        0.6403  0.0185\n",
      "     64        0.6699       0.6704        0.6402  0.0230\n",
      "     65        0.6644       0.6704        0.6399  0.0180\n",
      "     66        0.6626       0.6704        0.6394  0.0195\n",
      "     67        0.6627       0.6704        0.6392  0.0210\n",
      "     68        0.6605       0.6704        0.6388  0.0195\n",
      "     69        0.6644       0.6704        0.6386  0.0269\n",
      "     70        0.6660       0.6704        0.6382  0.0215\n",
      "     71        0.6629       0.6704        0.6378  0.0240\n",
      "     72        0.6644       0.6704        0.6375  0.0245\n",
      "     73        0.6657       0.6704        0.6372  0.0235\n",
      "     74        0.6631       0.6704        0.6369  0.0260\n",
      "     75        0.6618       0.6704        0.6365  0.0215\n",
      "     76        0.6612       0.6704        0.6362  0.0185\n",
      "     77        0.6619       0.6704        0.6359  0.0180\n",
      "     78        0.6597       0.6704        0.6354  0.0210\n",
      "     79        0.6566       0.6704        0.6349  0.0190\n",
      "     80        0.6632       0.6704        0.6346  0.0185\n",
      "     81        0.6587       0.6704        0.6343  0.0235\n",
      "     82        0.6589       0.6704        0.6338  0.0190\n",
      "     83        0.6588       0.6704        0.6334  0.0160\n",
      "     84        0.6594       0.6704        0.6328  0.0185\n",
      "     85        0.6643       0.6704        0.6326  0.0220\n",
      "     86        0.6575       0.6704        0.6322  0.0210\n",
      "     87        0.6589       0.6704        0.6319  0.0180\n",
      "     88        0.6584       0.6704        0.6316  0.0205\n",
      "     89        0.6574       0.6704        0.6311  0.0210\n",
      "     90        0.6585       0.6704        0.6308  0.0205\n",
      "     91        0.6522       0.6704        0.6302  0.0175\n",
      "     92        0.6507       0.6704        0.6295  0.0205\n",
      "     93        0.6548       0.6704        0.6289  0.0220\n",
      "     94        0.6536       0.6704        0.6282  0.0195\n",
      "     95        0.6526       0.6704        0.6277  0.0170\n",
      "     96        0.6478       0.6704        0.6268  0.0220\n",
      "     97        0.6548       0.6704        0.6262  0.0220\n",
      "     98        0.6492       0.6704        0.6256  0.0205\n",
      "     99        0.6515       0.6704        0.6249  0.0319\n",
      "    100        0.6534       0.6704        0.6244  0.0185\n",
      "    101        0.6559       0.6704        0.6238  0.0225\n",
      "    102        0.6536       0.6704        0.6233  0.0215\n",
      "    103        0.6466       0.6704        0.6225  0.0200\n",
      "    104        0.6496       0.6704        0.6216  0.0195\n",
      "    105        0.6466       0.6704        0.6208  0.0205\n",
      "    106        0.6516       0.6704        0.6203  0.0195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    107        0.6512       0.6704        0.6196  0.0210\n",
      "    108        0.6600       0.6704        0.6194  0.0200\n",
      "    109        0.6427       0.6704        0.6186  0.0200\n",
      "    110        0.6519       0.6704        0.6181  0.0180\n",
      "    111        0.6358       0.6704        0.6169  0.0180\n",
      "    112        0.6501       0.6704        0.6164  0.0205\n",
      "    113        0.6434       0.6704        0.6155  0.0195\n",
      "    114        0.6409       0.6704        0.6144  0.0185\n",
      "    115        0.6399       0.6704        0.6136  0.0210\n",
      "    116        0.6430       0.6704        0.6128  0.0190\n",
      "    117        0.6374       0.6704        0.6118  0.0180\n",
      "    118        0.6368       0.6704        0.6108  0.0190\n",
      "    119        0.6535       0.6704        0.6104  0.0205\n",
      "    120        0.6338       0.6704        0.6093  0.0195\n",
      "    121        0.6377       0.6704        0.6084  0.0225\n",
      "    122        0.6298       0.6704        0.6071  0.0185\n",
      "    123        0.6393       0.6704        0.6063  0.0200\n",
      "    124        0.6252       0.6704        0.6050  0.0190\n",
      "    125        0.6335       0.6704        0.6043  0.0215\n",
      "    126        0.6394       0.6704        0.6037  0.0205\n",
      "    127        0.6371       0.6704        0.6030  0.0175\n",
      "    128        0.6365       0.6704        0.6024  0.0185\n",
      "    129        0.6296       0.6704        0.6012  0.0195\n",
      "    130        0.6347       0.6704        0.6001  0.0190\n",
      "    131        0.6273       0.6704        0.5990  0.0190\n",
      "    132        0.6361       0.6704        0.5984  0.0200\n",
      "    133        0.6283       0.6704        0.5975  0.0160\n",
      "    134        0.6290       0.6704        0.5967  0.0195\n",
      "    135        0.6313       0.6704        0.5956  0.0190\n",
      "    136        0.6249       0.6816        0.5943  0.0210\n",
      "    137        0.6238       0.6816        0.5931  0.0225\n",
      "    138        0.6079       0.6816        0.5915  0.0200\n",
      "    139        0.6311       0.6816        0.5907  0.0200\n",
      "    140        0.6167       0.6927        0.5895  0.0175\n",
      "    141        0.6140       0.6983        0.5882  0.0190\n",
      "    142        0.6123       0.7095        0.5869  0.0200\n",
      "    143        0.6266       0.7095        0.5862  0.0220\n",
      "    144        0.6107       0.7095        0.5849  0.0220\n",
      "    145        0.6145       0.7151        0.5835  0.0190\n",
      "    146        0.6160       0.7207        0.5826  0.0210\n",
      "    147        0.6275       0.7207        0.5819  0.0215\n",
      "    148        0.6092       0.7318        0.5808  0.0185\n",
      "    149        0.6167       0.7263        0.5799  0.0185\n",
      "    150        0.6147       0.7318        0.5789  0.0170\n",
      "    151        0.6118       0.7318        0.5778  0.0190\n",
      "    152        0.6088       0.7318        0.5765  0.0220\n",
      "    153        0.6119       0.7318        0.5754  0.0185\n",
      "    154        0.6125       0.7318        0.5745  0.0175\n",
      "    155        0.6145       0.7318        0.5736  0.0190\n",
      "    156        0.6146       0.7318        0.5729  0.0200\n",
      "    157        0.6249       0.7318        0.5724  0.0269\n",
      "    158        0.6061       0.7318        0.5711  0.0195\n",
      "    159        0.6084       0.7318        0.5699  0.0190\n",
      "    160        0.6029       0.7542        0.5688  0.0195\n",
      "    161        0.6002       0.7598        0.5674  0.0205\n",
      "    162        0.6020       0.7654        0.5665  0.0250\n",
      "    163        0.5954       0.7709        0.5655  0.0205\n",
      "    164        0.5941       0.7877        0.5642  0.0235\n",
      "    165        0.5915       0.7877        0.5626  0.0240\n",
      "    166        0.6017       0.7877        0.5620  0.0230\n",
      "    167        0.5847       0.7877        0.5604  0.0215\n",
      "    168        0.6077       0.7877        0.5599  0.0230\n",
      "    169        0.6124       0.7877        0.5593  0.0259\n",
      "    170        0.5927       0.7933        0.5584  0.0230\n",
      "    171        0.5933       0.7933        0.5572  0.0215\n",
      "    172        0.6034       0.7933        0.5566  0.0180\n",
      "    173        0.5927       0.7933        0.5551  0.0165\n",
      "    174        0.5980       0.7933        0.5545  0.0255\n",
      "    175        0.6110       0.7933        0.5541  0.0260\n",
      "    176        0.6037       0.7933        0.5534  0.0235\n",
      "    177        0.6039       0.7933        0.5526  0.0265\n",
      "    178        0.5871       0.7933        0.5517  0.0255\n",
      "    179        0.5901       0.7933        0.5507  0.0245\n",
      "    180        0.5864       0.7821        0.5497  0.0195\n",
      "    181        0.6096       0.7877        0.5495  0.0175\n",
      "    182        0.5811       0.7877        0.5478  0.0225\n",
      "    183        0.5811       0.7877        0.5465  0.0274\n",
      "    184        0.5990       0.7877        0.5461  0.0235\n",
      "    185        0.5902       0.7933        0.5452  0.0240\n",
      "    186        0.6013       0.7933        0.5451  0.0250\n",
      "    187        0.5953       0.7933        0.5442  0.0220\n",
      "    188        0.5947       0.7933        0.5431  0.0180\n",
      "    189        0.5987       0.7933        0.5430  0.0220\n",
      "    190        0.5830       0.7933        0.5420  0.0235\n",
      "    191        0.5853       0.7933        0.5409  0.0210\n",
      "    192        0.5840       0.7989        0.5404  0.0195\n",
      "    193        0.5850       0.7933        0.5396  0.0215\n",
      "    194        0.5868       0.7989        0.5387  0.0240\n",
      "    195        0.5872       0.7989        0.5380  0.0239\n",
      "    196        0.5949       0.7989        0.5372  0.0225\n",
      "    197        0.5927       0.7989        0.5369  0.0240\n",
      "    198        0.5845       0.7989        0.5368  0.0245\n",
      "    199        0.5825       0.7989        0.5358  0.0220\n",
      "    200        0.5838       0.7989        0.5350  0.0240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.dataset import CVSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import skorch\n",
    "print(skorch.__version__)\n",
    "model = NeuralNetClassifier(\n",
    "        ClassifierModule,\n",
    "        max_epochs=200,\n",
    "        lr=0.02,\n",
    "        train_split=CVSplit(5),\n",
    "        #     device='cuda',  # uncomment this to train with CUDA\n",
    "    )\n",
    "   \n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "Y = Y_train.astype(np.int64)\n",
    "\n",
    "params = {\n",
    "        'lr': [0.02],\n",
    "        'max_epochs': [200],\n",
    "        'module__num_units': [20],\n",
    "}\n",
    "gs = GridSearchCV(model, params, refit=True, cv=2, scoring='accuracy')\n",
    "gs.fit(X, Y)\n",
    "print(gs.best_score_, gs.best_params_)\n",
    "gs.estimator.set_params(**gs.best_params_).fit(X, Y)\n",
    "\n",
    "predict_Survived_skorch = pd.Series(gs.predict(X_test.astype(np.float32)), name='Survived')\n",
    "skorch_result = pd.concat([IDtest, predict_Survived_skorch], axis=1)\n",
    "skorch_result.to_csv('skorch_result.csv', index=False)\n",
    "skorch_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "1、[Pytorch data_loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#)\n",
    "\n",
    "2、[Pytorch_transfer_learning_tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "3、[skorch basic usage](https://nbviewer.jupyter.org/github/dnouri/skorch/blob/master/notebooks/Basic_Usage.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
