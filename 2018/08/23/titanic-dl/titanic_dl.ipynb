{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用神经网络解决Titanic问题，主要用两种实现，一种是纯pytorch实现神经网络的搭建，另一种是使用skorch包装好的以pytorch为后端的API实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据文件，这里省去了特征工程的部分，特征采用之前特征工程提取出的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_None</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex  Title  Family_size  Family_Survival  \\\n",
       "0            1       3    0      0            2              0.5   \n",
       "1            2       1    1      2            2              0.5   \n",
       "2            3       3    1      1            1              0.5   \n",
       "3            4       1    1      2            2              0.0   \n",
       "4            5       3    0      0            1              0.5   \n",
       "\n",
       "   FareBin_Code  AgeBin_Code  Embarked_C  Embarked_None  Embarked_Q  \\\n",
       "0             0            2           0              0           0   \n",
       "1             4            3           1              0           0   \n",
       "2             1            2           0              0           0   \n",
       "3             4            3           0              0           0   \n",
       "4             1            3           0              0           0   \n",
       "\n",
       "   Embarked_S  Survived  \n",
       "0           1       0.0  \n",
       "1           0       1.0  \n",
       "2           1       1.0  \n",
       "3           1       1.0  \n",
       "4           1       0.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('train_features.csv')\n",
    "test=pd.read_csv('test_features.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId        891 non-null int64\n",
      "Pclass             891 non-null int64\n",
      "Sex                891 non-null int64\n",
      "Title              891 non-null int64\n",
      "Family_size        891 non-null int64\n",
      "Family_Survival    891 non-null float64\n",
      "FareBin_Code       891 non-null int64\n",
      "AgeBin_Code        891 non-null int64\n",
      "Embarked_C         891 non-null int64\n",
      "Embarked_None      891 non-null int64\n",
      "Embarked_Q         891 non-null int64\n",
      "Embarked_S         891 non-null int64\n",
      "Survived           891 non-null float64\n",
      "dtypes: float64(2), int64(11)\n",
      "memory usage: 90.6 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_None</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>1.904602</td>\n",
       "      <td>0.519641</td>\n",
       "      <td>1.985410</td>\n",
       "      <td>2.433221</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.972450</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>1.411355</td>\n",
       "      <td>1.370957</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.047351</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.486592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Sex       Title  Family_size  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000   891.000000   \n",
       "mean    446.000000    2.308642    0.352413    0.686869     1.904602   \n",
       "std     257.353842    0.836071    0.477990    0.972450     1.613459   \n",
       "min       1.000000    1.000000    0.000000    0.000000     1.000000   \n",
       "25%     223.500000    2.000000    0.000000    0.000000     1.000000   \n",
       "50%     446.000000    3.000000    0.000000    0.000000     1.000000   \n",
       "75%     668.500000    3.000000    1.000000    1.000000     2.000000   \n",
       "max     891.000000    3.000000    1.000000    4.000000    11.000000   \n",
       "\n",
       "       Family_Survival  FareBin_Code  AgeBin_Code  Embarked_C  Embarked_None  \\\n",
       "count       891.000000    891.000000   891.000000  891.000000     891.000000   \n",
       "mean          0.519641      1.985410     2.433221    0.188552       0.002245   \n",
       "std           0.323961      1.411355     1.370957    0.391372       0.047351   \n",
       "min           0.000000      0.000000     0.000000    0.000000       0.000000   \n",
       "25%           0.500000      1.000000     2.000000    0.000000       0.000000   \n",
       "50%           0.500000      2.000000     2.000000    0.000000       0.000000   \n",
       "75%           0.500000      3.000000     3.000000    0.000000       0.000000   \n",
       "max           1.000000      4.000000     7.000000    1.000000       1.000000   \n",
       "\n",
       "       Embarked_Q  Embarked_S    Survived  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean     0.086420    0.722783    0.383838  \n",
       "std      0.281141    0.447876    0.486592  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    1.000000    0.000000  \n",
       "75%      0.000000    1.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.drop(['PassengerId','Survived'],axis=1).as_matrix()\n",
    "Y_train=train['Survived'].astype(int).as_matrix()\n",
    "\n",
    "X_test=test.drop(['PassengerId','Survived'],axis=1).as_matrix()\n",
    "IDtest=test['PassengerId']\n",
    "\n",
    "# scalar\n",
    "scaler=MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Pytorch搭建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据载入和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicFeaturesDataset(Dataset):\n",
    "    def __init__(self,X,y=None,transform=None):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.y is not None:\n",
    "            sample={'X':self.X[idx],'y':self.y[idx]}\n",
    "        else:\n",
    "            sample={'X':self.X[idx]}\n",
    "        if self.transform is not None:\n",
    "            sample=self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform class\n",
    "class ToTensor(object):\n",
    "    def __call__(self,sample):\n",
    "        if 'y' in sample.keys():\n",
    "            X,y=sample['X'],sample['y']\n",
    "            return {\n",
    "                'X':torch.from_numpy(X.astype(np.float32)),\n",
    "                'y':torch.squeeze(torch.from_numpy(np.array([y.astype(np.int64)])))\n",
    "            }\n",
    "        else:\n",
    "            X=sample['X']\n",
    "            return {\n",
    "                'X':torch.from_numpy(X.astype(np.float32))\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "# 自定义Model需要继承nn.Module\n",
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(self,D_in=11,D_out=2,num_units=20,nonlin=F.relu,dropout=0.5):\n",
    "        super(ClassifierModule,self).__init__()\n",
    "        self.num_units=num_units\n",
    "        self.nonlin=nonlin\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear1=nn.Linear(D_in,num_units)\n",
    "        self.linear2=nn.Linear(num_units,10)\n",
    "        self.output=nn.Linear(10,2)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.nonlin(self.linear1(X))\n",
    "        X=self.dropout(X)\n",
    "        X=self.nonlin(self.linear2(X))\n",
    "        X=self.dropout(X)\n",
    "        X=self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train_model(model,criterion,optimiizer,scheduler,dataset_sizes,num_epochs=100,device='cpu'):\n",
    "    start=time.time()\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    best_acc=0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss=0.0\n",
    "            running_corrects=0\n",
    "            for sample_batches in dataloaders[phase]:\n",
    "                inputs=sample_batches['X'].to(device)\n",
    "                labels=sample_batches['y'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs=model(inputs)\n",
    "                    _,preds=torch.max(outputs,1)\n",
    "                    loss=criterion(outputs,labels)\n",
    "                    \n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    running_loss+=loss.item()*inputs.size(0)\n",
    "                    running_corrects+=torch.sum(preds==labels.data)\n",
    "            epoch_loss=running_loss/dataset_sizes[phase]\n",
    "            epoch_acc=running_corrects.double()/dataset_sizes[phase]\n",
    "            print('{} loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss,epoch_acc))\n",
    "            \n",
    "            if phase=='val' and epoch_acc>best_acc:\n",
    "                best_acc=epoch_acc\n",
    "                best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    time_elapsed=time.time()-start\n",
    "    print('Trainning complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed//60,time_elapsed%60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and train\n",
    "train_dataset_len=X_train.shape[0]\n",
    "train_len=train_dataset_len*4//5\n",
    "\n",
    "transformed_datasets = {\n",
    "        'train': TitanicFeaturesDataset(X_train[:train_len],Y_train[:train_len], transform=transforms.Compose([ToTensor()])),\n",
    "        'val': TitanicFeaturesDataset(X_train[train_len:], Y_train[train_len:], transform=transforms.Compose([ToTensor()])),\n",
    "        'test':TitanicFeaturesDataset(X_test,transform=transforms.Compose([ToTensor()]))\n",
    "        }\n",
    "\n",
    "dataloaders = {x: DataLoader(transformed_datasets[x], batch_size=16,\n",
    "                                 shuffle=True, num_workers=0)\n",
    "                   for x in ['train', 'val']}\n",
    "dataloaders['test']=DataLoader(transformed_datasets['test'],batch_size=16,shuffle=False,num_workers=0)\n",
    "\n",
    "dataset_sizes = {x: len(transformed_datasets[x]) for x in ['train', 'val','test']}\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train loss: 0.6776 Acc: 0.5688\n",
      "val loss: 0.6443 Acc: 0.6425\n",
      "Epoch 1/99\n",
      "----------\n",
      "train loss: 0.6423 Acc: 0.6096\n",
      "val loss: 0.5799 Acc: 0.6425\n",
      "Epoch 2/99\n",
      "----------\n",
      "train loss: 0.6237 Acc: 0.6180\n",
      "val loss: 0.5448 Acc: 0.8268\n",
      "Epoch 3/99\n",
      "----------\n",
      "train loss: 0.6081 Acc: 0.6840\n",
      "val loss: 0.4879 Acc: 0.8436\n",
      "Epoch 4/99\n",
      "----------\n",
      "train loss: 0.5794 Acc: 0.7008\n",
      "val loss: 0.4660 Acc: 0.8547\n",
      "Epoch 5/99\n",
      "----------\n",
      "train loss: 0.5622 Acc: 0.7205\n",
      "val loss: 0.4254 Acc: 0.8659\n",
      "Epoch 6/99\n",
      "----------\n",
      "train loss: 0.5367 Acc: 0.7275\n",
      "val loss: 0.3823 Acc: 0.8771\n",
      "Epoch 7/99\n",
      "----------\n",
      "train loss: 0.5045 Acc: 0.7570\n",
      "val loss: 0.3828 Acc: 0.8715\n",
      "Epoch 8/99\n",
      "----------\n",
      "train loss: 0.5299 Acc: 0.7542\n",
      "val loss: 0.3919 Acc: 0.8492\n",
      "Epoch 9/99\n",
      "----------\n",
      "train loss: 0.5182 Acc: 0.7528\n",
      "val loss: 0.3942 Acc: 0.8547\n",
      "Epoch 10/99\n",
      "----------\n",
      "train loss: 0.5024 Acc: 0.7683\n",
      "val loss: 0.3924 Acc: 0.8492\n",
      "Epoch 11/99\n",
      "----------\n",
      "train loss: 0.5073 Acc: 0.7626\n",
      "val loss: 0.3918 Acc: 0.8492\n",
      "Epoch 12/99\n",
      "----------\n",
      "train loss: 0.4899 Acc: 0.7725\n",
      "val loss: 0.3909 Acc: 0.8547\n",
      "Epoch 13/99\n",
      "----------\n",
      "train loss: 0.4917 Acc: 0.7542\n",
      "val loss: 0.3848 Acc: 0.8547\n",
      "Epoch 14/99\n",
      "----------\n",
      "train loss: 0.4891 Acc: 0.7669\n",
      "val loss: 0.3842 Acc: 0.8547\n",
      "Epoch 15/99\n",
      "----------\n",
      "train loss: 0.4961 Acc: 0.7711\n",
      "val loss: 0.3842 Acc: 0.8547\n",
      "Epoch 16/99\n",
      "----------\n",
      "train loss: 0.4998 Acc: 0.7458\n",
      "val loss: 0.3838 Acc: 0.8547\n",
      "Epoch 17/99\n",
      "----------\n",
      "train loss: 0.4955 Acc: 0.7683\n",
      "val loss: 0.3839 Acc: 0.8547\n",
      "Epoch 18/99\n",
      "----------\n",
      "train loss: 0.5185 Acc: 0.7374\n",
      "val loss: 0.3835 Acc: 0.8547\n",
      "Epoch 19/99\n",
      "----------\n",
      "train loss: 0.5158 Acc: 0.7528\n",
      "val loss: 0.3836 Acc: 0.8547\n",
      "Epoch 20/99\n",
      "----------\n",
      "train loss: 0.5088 Acc: 0.7500\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 21/99\n",
      "----------\n",
      "train loss: 0.4938 Acc: 0.7767\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 22/99\n",
      "----------\n",
      "train loss: 0.4966 Acc: 0.7683\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 23/99\n",
      "----------\n",
      "train loss: 0.4860 Acc: 0.7598\n",
      "val loss: 0.3835 Acc: 0.8603\n",
      "Epoch 24/99\n",
      "----------\n",
      "train loss: 0.5055 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 25/99\n",
      "----------\n",
      "train loss: 0.4947 Acc: 0.7725\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 26/99\n",
      "----------\n",
      "train loss: 0.5083 Acc: 0.7472\n",
      "val loss: 0.3835 Acc: 0.8603\n",
      "Epoch 27/99\n",
      "----------\n",
      "train loss: 0.4968 Acc: 0.7598\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 28/99\n",
      "----------\n",
      "train loss: 0.5061 Acc: 0.7500\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 29/99\n",
      "----------\n",
      "train loss: 0.4903 Acc: 0.7711\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 30/99\n",
      "----------\n",
      "train loss: 0.5075 Acc: 0.7640\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 31/99\n",
      "----------\n",
      "train loss: 0.5103 Acc: 0.7739\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 32/99\n",
      "----------\n",
      "train loss: 0.4889 Acc: 0.7542\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 33/99\n",
      "----------\n",
      "train loss: 0.5166 Acc: 0.7416\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 34/99\n",
      "----------\n",
      "train loss: 0.5172 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 35/99\n",
      "----------\n",
      "train loss: 0.4923 Acc: 0.7739\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 36/99\n",
      "----------\n",
      "train loss: 0.5216 Acc: 0.7598\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 37/99\n",
      "----------\n",
      "train loss: 0.5353 Acc: 0.7444\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 38/99\n",
      "----------\n",
      "train loss: 0.5114 Acc: 0.7289\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 39/99\n",
      "----------\n",
      "train loss: 0.4990 Acc: 0.7711\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 40/99\n",
      "----------\n",
      "train loss: 0.4959 Acc: 0.7781\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 41/99\n",
      "----------\n",
      "train loss: 0.4988 Acc: 0.7683\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 42/99\n",
      "----------\n",
      "train loss: 0.5305 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 43/99\n",
      "----------\n",
      "train loss: 0.4831 Acc: 0.7739\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 44/99\n",
      "----------\n",
      "train loss: 0.5284 Acc: 0.7612\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 45/99\n",
      "----------\n",
      "train loss: 0.5027 Acc: 0.7556\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 46/99\n",
      "----------\n",
      "train loss: 0.4873 Acc: 0.7612\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 47/99\n",
      "----------\n",
      "train loss: 0.5027 Acc: 0.7697\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 48/99\n",
      "----------\n",
      "train loss: 0.5203 Acc: 0.7458\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 49/99\n",
      "----------\n",
      "train loss: 0.4914 Acc: 0.7725\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 50/99\n",
      "----------\n",
      "train loss: 0.4934 Acc: 0.7444\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 51/99\n",
      "----------\n",
      "train loss: 0.4998 Acc: 0.7612\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 52/99\n",
      "----------\n",
      "train loss: 0.5202 Acc: 0.7669\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 53/99\n",
      "----------\n",
      "train loss: 0.5076 Acc: 0.7528\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 54/99\n",
      "----------\n",
      "train loss: 0.5139 Acc: 0.7486\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 55/99\n",
      "----------\n",
      "train loss: 0.5051 Acc: 0.7725\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 56/99\n",
      "----------\n",
      "train loss: 0.5007 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 57/99\n",
      "----------\n",
      "train loss: 0.5096 Acc: 0.7458\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 58/99\n",
      "----------\n",
      "train loss: 0.4793 Acc: 0.7907\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 59/99\n",
      "----------\n",
      "train loss: 0.5058 Acc: 0.7654\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 60/99\n",
      "----------\n",
      "train loss: 0.5072 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 61/99\n",
      "----------\n",
      "train loss: 0.5183 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 62/99\n",
      "----------\n",
      "train loss: 0.5117 Acc: 0.7640\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 63/99\n",
      "----------\n",
      "train loss: 0.4815 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 64/99\n",
      "----------\n",
      "train loss: 0.4930 Acc: 0.7669\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 65/99\n",
      "----------\n",
      "train loss: 0.5238 Acc: 0.7697\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 66/99\n",
      "----------\n",
      "train loss: 0.4887 Acc: 0.7837\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 67/99\n",
      "----------\n",
      "train loss: 0.5032 Acc: 0.7584\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 68/99\n",
      "----------\n",
      "train loss: 0.4833 Acc: 0.7879\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 69/99\n",
      "----------\n",
      "train loss: 0.5024 Acc: 0.7837\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 70/99\n",
      "----------\n",
      "train loss: 0.5019 Acc: 0.7514\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 71/99\n",
      "----------\n",
      "train loss: 0.4858 Acc: 0.7823\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 72/99\n",
      "----------\n",
      "train loss: 0.5149 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 73/99\n",
      "----------\n",
      "train loss: 0.5031 Acc: 0.7500\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 74/99\n",
      "----------\n",
      "train loss: 0.5208 Acc: 0.7542\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 75/99\n",
      "----------\n",
      "train loss: 0.5057 Acc: 0.7612\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 76/99\n",
      "----------\n",
      "train loss: 0.5028 Acc: 0.7809\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 77/99\n",
      "----------\n",
      "train loss: 0.5004 Acc: 0.7556\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 78/99\n",
      "----------\n",
      "train loss: 0.5188 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 79/99\n",
      "----------\n",
      "train loss: 0.5336 Acc: 0.7416\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 80/99\n",
      "----------\n",
      "train loss: 0.5152 Acc: 0.7654\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 81/99\n",
      "----------\n",
      "train loss: 0.4863 Acc: 0.7725\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 82/99\n",
      "----------\n",
      "train loss: 0.4961 Acc: 0.7697\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 83/99\n",
      "----------\n",
      "train loss: 0.5160 Acc: 0.7542\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 84/99\n",
      "----------\n",
      "train loss: 0.5017 Acc: 0.7598\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 85/99\n",
      "----------\n",
      "train loss: 0.5081 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 86/99\n",
      "----------\n",
      "train loss: 0.4978 Acc: 0.7654\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 87/99\n",
      "----------\n",
      "train loss: 0.4830 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 88/99\n",
      "----------\n",
      "train loss: 0.5010 Acc: 0.7570\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 89/99\n",
      "----------\n",
      "train loss: 0.4944 Acc: 0.7851\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 90/99\n",
      "----------\n",
      "train loss: 0.5036 Acc: 0.7542\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 91/99\n",
      "----------\n",
      "train loss: 0.5078 Acc: 0.7669\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 92/99\n",
      "----------\n",
      "train loss: 0.5085 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 93/99\n",
      "----------\n",
      "train loss: 0.4892 Acc: 0.7542\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 94/99\n",
      "----------\n",
      "train loss: 0.5082 Acc: 0.7598\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 95/99\n",
      "----------\n",
      "train loss: 0.5221 Acc: 0.7711\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 96/99\n",
      "----------\n",
      "train loss: 0.4791 Acc: 0.7753\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 97/99\n",
      "----------\n",
      "train loss: 0.4965 Acc: 0.7598\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 98/99\n",
      "----------\n",
      "train loss: 0.4819 Acc: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.3836 Acc: 0.8603\n",
      "Epoch 99/99\n",
      "----------\n",
      "train loss: 0.4981 Acc: 0.7626\n",
      "val loss: 0.3836 Acc: 0.8603\n",
      "Trainning complete in 0m 9s\n",
      "Best val Acc: 0.877095\n"
     ]
    }
   ],
   "source": [
    "model=ClassifierModule()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=2e-2,momentum=0.9)\n",
    "exp_lr_scheduler=lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)\n",
    "\n",
    "model=train_model(model,criterion,optimizer,exp_lr_scheduler,dataset_sizes,num_epochs=100,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model and reload it\n",
    "torch.save(model.state_dict(),'pytorch_model.pth')\n",
    "reloaded_model=ClassifierModule()\n",
    "reloaded_model.load_state_dict(torch.load('pytorch_model.pth'))\n",
    "# predicts on  test dataset\n",
    "\n",
    "reloaded_model.eval()\n",
    "final_predicts=[]\n",
    "with torch.no_grad():\n",
    "    for samples in dataloaders['test']:\n",
    "        inputs=samples['X'].to(device)\n",
    "        outputs=reloaded_model(inputs)\n",
    "        _,preds=torch.max(outputs,1)\n",
    "        for y_predict in list(preds.numpy()):\n",
    "            final_predicts.append(y_predict)\n",
    "final_predicts=np.array(final_predicts).reshape(-1,)\n",
    "predict_survived_pytorch=pd.Series(final_predicts,name='Survived')\n",
    "pytorch_result=pd.concat([IDtest,predict_survived_pytorch],axis=1)\n",
    "pytorch_result.to_csv('pytorch_result.csv',index=False)\n",
    "pytorch_result.head()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用skorch搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6876       0.6404        0.6841  0.0120\n",
      "      2        0.6834       0.6404        0.6816  0.0110\n",
      "      3        0.6788       0.6404        0.6791  0.0165\n",
      "      4        0.6802       0.6404        0.6769  0.0140\n",
      "      5        0.6817       0.6404        0.6748  0.0140\n",
      "      6        0.6749       0.6404        0.6727  0.0120\n",
      "      7        0.6718       0.6404        0.6708  0.0135\n",
      "      8        0.6759       0.6404        0.6690  0.0135\n",
      "      9        0.6729       0.6404        0.6673  0.0135\n",
      "     10        0.6675       0.6404        0.6657  0.0150\n",
      "     11        0.6768       0.6404        0.6643  0.0175\n",
      "     12        0.6694       0.6404        0.6628  0.0135\n",
      "     13        0.6613       0.6404        0.6614  0.0175\n",
      "     14        0.6581       0.6404        0.6600  0.0175\n",
      "     15        0.6718       0.6404        0.6590  0.0155\n",
      "     16        0.6615       0.6404        0.6578  0.0135\n",
      "     17        0.6675       0.6404        0.6568  0.0105\n",
      "     18        0.6644       0.6404        0.6557  0.0135\n",
      "     19        0.6622       0.6404        0.6547  0.0110\n",
      "     20        0.6631       0.6404        0.6538  0.0105\n",
      "     21        0.6620       0.6404        0.6529  0.0135\n",
      "     22        0.6537       0.6404        0.6519  0.0145\n",
      "     23        0.6514       0.6404        0.6510  0.0185\n",
      "     24        0.6626       0.6404        0.6502  0.0195\n",
      "     25        0.6596       0.6404        0.6494  0.0135\n",
      "     26        0.6668       0.6404        0.6487  0.0095\n",
      "     27        0.6535       0.6404        0.6479  0.0120\n",
      "     28        0.6558       0.6404        0.6472  0.0130\n",
      "     29        0.6517       0.6404        0.6463  0.0125\n",
      "     30        0.6574       0.6404        0.6456  0.0110\n",
      "     31        0.6547       0.6404        0.6449  0.0135\n",
      "     32        0.6491       0.6404        0.6440  0.0135\n",
      "     33        0.6467       0.6404        0.6431  0.0150\n",
      "     34        0.6480       0.6404        0.6423  0.0135\n",
      "     35        0.6564       0.6404        0.6417  0.0115\n",
      "     36        0.6573       0.6404        0.6411  0.0095\n",
      "     37        0.6543       0.6404        0.6405  0.0105\n",
      "     38        0.6522       0.6404        0.6397  0.0095\n",
      "     39        0.6439       0.6404        0.6388  0.0100\n",
      "     40        0.6426       0.6404        0.6381  0.0125\n",
      "     41        0.6561       0.6404        0.6375  0.0170\n",
      "     42        0.6446       0.6404        0.6368  0.0125\n",
      "     43        0.6465       0.6404        0.6360  0.0150\n",
      "     44        0.6509       0.6404        0.6353  0.0135\n",
      "     45        0.6491       0.6404        0.6347  0.0130\n",
      "     46        0.6423       0.6404        0.6338  0.0115\n",
      "     47        0.6480       0.6404        0.6332  0.0150\n",
      "     48        0.6325       0.6404        0.6323  0.0180\n",
      "     49        0.6529       0.6404        0.6316  0.0155\n",
      "     50        0.6480       0.6404        0.6310  0.0200\n",
      "     51        0.6377       0.6404        0.6303  0.0110\n",
      "     52        0.6368       0.6404        0.6295  0.0165\n",
      "     53        0.6353       0.6404        0.6287  0.0125\n",
      "     54        0.6456       0.6404        0.6281  0.0120\n",
      "     55        0.6340       0.6404        0.6274  0.0170\n",
      "     56        0.6388       0.6404        0.6266  0.0175\n",
      "     57        0.6408       0.6404        0.6259  0.0150\n",
      "     58        0.6349       0.6404        0.6252  0.0205\n",
      "     59        0.6386       0.6404        0.6245  0.0140\n",
      "     60        0.6344       0.6404        0.6234  0.0205\n",
      "     61        0.6352       0.6404        0.6227  0.0130\n",
      "     62        0.6432       0.6404        0.6220  0.0130\n",
      "     63        0.6368       0.6404        0.6213  0.0105\n",
      "     64        0.6316       0.6404        0.6205  0.0110\n",
      "     65        0.6225       0.6404        0.6195  0.0140\n",
      "     66        0.6302       0.6404        0.6187  0.0100\n",
      "     67        0.6294       0.6404        0.6179  0.0130\n",
      "     68        0.6303       0.6404        0.6171  0.0140\n",
      "     69        0.6363       0.6404        0.6163  0.0110\n",
      "     70        0.6283       0.6404        0.6156  0.0115\n",
      "     71        0.6280       0.6404        0.6148  0.0140\n",
      "     72        0.6287       0.6404        0.6140  0.0105\n",
      "     73        0.6328       0.6404        0.6131  0.0130\n",
      "     74        0.6308       0.6404        0.6124  0.0135\n",
      "     75        0.6174       0.6404        0.6115  0.0150\n",
      "     76        0.6317       0.6404        0.6107  0.0120\n",
      "     77        0.6362       0.6404        0.6099  0.0130\n",
      "     78        0.6225       0.6404        0.6091  0.0110\n",
      "     79        0.6199       0.6404        0.6082  0.0115\n",
      "     80        0.6185       0.6404        0.6073  0.0135\n",
      "     81        0.6229       0.6404        0.6064  0.0130\n",
      "     82        0.6239       0.6404        0.6056  0.0130\n",
      "     83        0.6290       0.6404        0.6047  0.0100\n",
      "     84        0.6255       0.6404        0.6039  0.0095\n",
      "     85        0.6300       0.6404        0.6031  0.0155\n",
      "     86        0.6175       0.6404        0.6020  0.0115\n",
      "     87        0.6085       0.6404        0.6009  0.0095\n",
      "     88        0.6209       0.6404        0.6000  0.0120\n",
      "     89        0.6346       0.6404        0.5994  0.0155\n",
      "     90        0.6295       0.6404        0.5986  0.0130\n",
      "     91        0.6173       0.6404        0.5977  0.0110\n",
      "     92        0.6071       0.6404        0.5966  0.0120\n",
      "     93        0.6188       0.6404        0.5957  0.0125\n",
      "     94        0.6164       0.6404        0.5947  0.0135\n",
      "     95        0.6138       0.6404        0.5937  0.0120\n",
      "     96        0.6054       0.6517        0.5925  0.0135\n",
      "     97        0.6202       0.6517        0.5917  0.0130\n",
      "     98        0.6119       0.6404        0.5907  0.0105\n",
      "     99        0.6197       0.6517        0.5898  0.0110\n",
      "    100        0.6179       0.6629        0.5888  0.0160\n",
      "    101        0.6058       0.6629        0.5878  0.0120\n",
      "    102        0.6093       0.6742        0.5868  0.0150\n",
      "    103        0.6134       0.6854        0.5858  0.0105\n",
      "    104        0.6102       0.6854        0.5849  0.0140\n",
      "    105        0.6076       0.6854        0.5838  0.0125\n",
      "    106        0.6080       0.6854        0.5827  0.0120\n",
      "    107        0.6048       0.6854        0.5816  0.0140\n",
      "    108        0.6210       0.6854        0.5808  0.0125\n",
      "    109        0.6123       0.6854        0.5799  0.0140\n",
      "    110        0.6037       0.6854        0.5787  0.0125\n",
      "    111        0.6016       0.6854        0.5778  0.0105\n",
      "    112        0.6043       0.6966        0.5765  0.0115\n",
      "    113        0.5982       0.7079        0.5754  0.0145\n",
      "    114        0.6025       0.7079        0.5743  0.0130\n",
      "    115        0.5935       0.7079        0.5731  0.0130\n",
      "    116        0.5984       0.7079        0.5719  0.0140\n",
      "    117        0.6004       0.7079        0.5707  0.0135\n",
      "    118        0.5847       0.7079        0.5692  0.0140\n",
      "    119        0.6014       0.7079        0.5681  0.0125\n",
      "    120        0.5929       0.7079        0.5669  0.0125\n",
      "    121        0.6014       0.7079        0.5659  0.0120\n",
      "    122        0.6119       0.7079        0.5651  0.0100\n",
      "    123        0.5867       0.7079        0.5638  0.0130\n",
      "    124        0.5952       0.7079        0.5627  0.0150\n",
      "    125        0.5869       0.7079        0.5614  0.0140\n",
      "    126        0.5904       0.7079        0.5602  0.0100\n",
      "    127        0.5893       0.7079        0.5591  0.0110\n",
      "    128        0.5884       0.7079        0.5579  0.0120\n",
      "    129        0.5883       0.7079        0.5565  0.0115\n",
      "    130        0.5964       0.7079        0.5554  0.0125\n",
      "    131        0.5875       0.7303        0.5540  0.0120\n",
      "    132        0.5993       0.7416        0.5532  0.0150\n",
      "    133        0.5876       0.7528        0.5521  0.0135\n",
      "    134        0.5889       0.7528        0.5511  0.0140\n",
      "    135        0.5668       0.7528        0.5494  0.0135\n",
      "    136        0.5716       0.7528        0.5482  0.0150\n",
      "    137        0.6063       0.7528        0.5474  0.0165\n",
      "    138        0.5715       0.7640        0.5460  0.0125\n",
      "    139        0.5801       0.7640        0.5448  0.0130\n",
      "    140        0.5752       0.7640        0.5432  0.0145\n",
      "    141        0.5883       0.7640        0.5421  0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    142        0.5872       0.7640        0.5410  0.0135\n",
      "    143        0.5785       0.7753        0.5397  0.0160\n",
      "    144        0.5877       0.7753        0.5389  0.0180\n",
      "    145        0.5702       0.7753        0.5378  0.0130\n",
      "    146        0.5794       0.7865        0.5369  0.0160\n",
      "    147        0.5627       0.8090        0.5355  0.0105\n",
      "    148        0.5617       0.8090        0.5343  0.0140\n",
      "    149        0.5665       0.8090        0.5330  0.0100\n",
      "    150        0.5657       0.8090        0.5320  0.0105\n",
      "    151        0.5763       0.8090        0.5309  0.0095\n",
      "    152        0.5792       0.8090        0.5300  0.0140\n",
      "    153        0.5647       0.8090        0.5289  0.0105\n",
      "    154        0.5547       0.8090        0.5274  0.0115\n",
      "    155        0.5838       0.8202        0.5264  0.0130\n",
      "    156        0.5688       0.8202        0.5252  0.0155\n",
      "    157        0.5574       0.8202        0.5238  0.0115\n",
      "    158        0.5820       0.8202        0.5231  0.0135\n",
      "    159        0.5476       0.8202        0.5215  0.0120\n",
      "    160        0.5681       0.8202        0.5203  0.0110\n",
      "    161        0.5652       0.8202        0.5194  0.0115\n",
      "    162        0.5509       0.8202        0.5181  0.0105\n",
      "    163        0.5567       0.8202        0.5170  0.0125\n",
      "    164        0.5378       0.8202        0.5153  0.0120\n",
      "    165        0.5857       0.8202        0.5147  0.0105\n",
      "    166        0.5482       0.8202        0.5136  0.0150\n",
      "    167        0.5706       0.8202        0.5127  0.0145\n",
      "    168        0.5868       0.8202        0.5123  0.0100\n",
      "    169        0.5547       0.8202        0.5112  0.0110\n",
      "    170        0.5537       0.8202        0.5101  0.0135\n",
      "    171        0.5401       0.8202        0.5089  0.0100\n",
      "    172        0.5705       0.8202        0.5078  0.0095\n",
      "    173        0.5407       0.8202        0.5066  0.0125\n",
      "    174        0.5583       0.8202        0.5058  0.0175\n",
      "    175        0.5451       0.8202        0.5048  0.0120\n",
      "    176        0.5704       0.8202        0.5041  0.0170\n",
      "    177        0.5422       0.8202        0.5028  0.0155\n",
      "    178        0.5402       0.8202        0.5017  0.0130\n",
      "    179        0.5587       0.8090        0.5009  0.0230\n",
      "    180        0.5580       0.8090        0.5001  0.0135\n",
      "    181        0.5394       0.8090        0.4990  0.0160\n",
      "    182        0.5637       0.8090        0.4982  0.0115\n",
      "    183        0.5380       0.8090        0.4970  0.0135\n",
      "    184        0.5381       0.8090        0.4959  0.0160\n",
      "    185        0.5451       0.8090        0.4950  0.0120\n",
      "    186        0.5305       0.8090        0.4934  0.0100\n",
      "    187        0.5393       0.8090        0.4925  0.0150\n",
      "    188        0.5659       0.8090        0.4921  0.0130\n",
      "    189        0.5297       0.8090        0.4908  0.0130\n",
      "    190        0.5408       0.8090        0.4899  0.0145\n",
      "    191        0.5489       0.8090        0.4892  0.0160\n",
      "    192        0.5194       0.7978        0.4880  0.0155\n",
      "    193        0.5126       0.7978        0.4868  0.0165\n",
      "    194        0.5241       0.7978        0.4855  0.0165\n",
      "    195        0.5368       0.7865        0.4846  0.0155\n",
      "    196        0.5366       0.7865        0.4837  0.0140\n",
      "    197        0.5563       0.7978        0.4832  0.0135\n",
      "    198        0.5456       0.7978        0.4825  0.0120\n",
      "    199        0.5293       0.7978        0.4817  0.0120\n",
      "    200        0.5385       0.7978        0.4809  0.0105\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6883       0.5667        0.6848  0.0075\n",
      "      2        0.6921       0.5667        0.6839  0.0140\n",
      "      3        0.6736       0.5667        0.6830  0.0120\n",
      "      4        0.6794       0.5667        0.6822  0.0155\n",
      "      5        0.6752       0.5667        0.6814  0.0120\n",
      "      6        0.6769       0.5667        0.6808  0.0115\n",
      "      7        0.6707       0.5667        0.6801  0.0105\n",
      "      8        0.6811       0.5667        0.6795  0.0135\n",
      "      9        0.6835       0.5667        0.6791  0.0095\n",
      "     10        0.6725       0.5667        0.6786  0.0125\n",
      "     11        0.6746       0.5667        0.6782  0.0140\n",
      "     12        0.6667       0.5667        0.6778  0.0110\n",
      "     13        0.6708       0.5667        0.6774  0.0180\n",
      "     14        0.6700       0.5667        0.6771  0.0120\n",
      "     15        0.6631       0.5667        0.6768  0.0150\n",
      "     16        0.6654       0.5667        0.6764  0.0130\n",
      "     17        0.6671       0.5667        0.6761  0.0090\n",
      "     18        0.6660       0.5667        0.6758  0.0105\n",
      "     19        0.6710       0.5667        0.6756  0.0130\n",
      "     20        0.6586       0.5667        0.6753  0.0110\n",
      "     21        0.6634       0.5667        0.6750  0.0155\n",
      "     22        0.6665       0.5667        0.6747  0.0120\n",
      "     23        0.6652       0.5667        0.6745  0.0120\n",
      "     24        0.6621       0.5667        0.6743  0.0100\n",
      "     25        0.6628       0.5667        0.6742  0.0110\n",
      "     26        0.6688       0.5667        0.6740  0.0140\n",
      "     27        0.6565       0.5667        0.6737  0.0105\n",
      "     28        0.6556       0.5667        0.6733  0.0145\n",
      "     29        0.6497       0.5667        0.6730  0.0155\n",
      "     30        0.6680       0.5667        0.6729  0.0150\n",
      "     31        0.6566       0.5667        0.6728  0.0135\n",
      "     32        0.6537       0.5667        0.6724  0.0125\n",
      "     33        0.6576       0.5667        0.6722  0.0105\n",
      "     34        0.6561       0.5667        0.6720  0.0115\n",
      "     35        0.6584       0.5667        0.6716  0.0140\n",
      "     36        0.6543       0.5667        0.6714  0.0135\n",
      "     37        0.6618       0.5667        0.6712  0.0110\n",
      "     38        0.6576       0.5667        0.6710  0.0130\n",
      "     39        0.6529       0.5667        0.6707  0.0125\n",
      "     40        0.6618       0.5667        0.6705  0.0150\n",
      "     41        0.6543       0.5667        0.6703  0.0105\n",
      "     42        0.6535       0.5667        0.6700  0.0155\n",
      "     43        0.6650       0.5667        0.6698  0.0155\n",
      "     44        0.6581       0.5667        0.6694  0.0135\n",
      "     45        0.6540       0.5667        0.6693  0.0125\n",
      "     46        0.6486       0.5667        0.6689  0.0185\n",
      "     47        0.6481       0.5667        0.6685  0.0145\n",
      "     48        0.6511       0.5667        0.6680  0.0145\n",
      "     49        0.6449       0.5667        0.6678  0.0140\n",
      "     50        0.6627       0.5667        0.6675  0.0145\n",
      "     51        0.6473       0.5667        0.6669  0.0145\n",
      "     52        0.6554       0.5667        0.6666  0.0115\n",
      "     53        0.6511       0.5667        0.6662  0.0105\n",
      "     54        0.6442       0.5667        0.6657  0.0120\n",
      "     55        0.6479       0.5667        0.6653  0.0130\n",
      "     56        0.6505       0.5667        0.6648  0.0130\n",
      "     57        0.6522       0.5667        0.6642  0.0130\n",
      "     58        0.6499       0.5667        0.6639  0.0140\n",
      "     59        0.6619       0.5667        0.6635  0.0155\n",
      "     60        0.6560       0.5667        0.6630  0.0120\n",
      "     61        0.6432       0.5667        0.6625  0.0145\n",
      "     62        0.6426       0.5667        0.6620  0.0125\n",
      "     63        0.6499       0.5667        0.6616  0.0110\n",
      "     64        0.6453       0.5667        0.6611  0.0120\n",
      "     65        0.6498       0.5667        0.6607  0.0135\n",
      "     66        0.6506       0.5667        0.6604  0.0130\n",
      "     67        0.6447       0.5667        0.6598  0.0140\n",
      "     68        0.6475       0.5667        0.6593  0.0120\n",
      "     69        0.6495       0.5667        0.6589  0.0150\n",
      "     70        0.6457       0.5667        0.6586  0.0150\n",
      "     71        0.6500       0.5667        0.6584  0.0100\n",
      "     72        0.6386       0.5667        0.6578  0.0150\n",
      "     73        0.6410       0.5667        0.6573  0.0145\n",
      "     74        0.6529       0.5667        0.6571  0.0120\n",
      "     75        0.6493       0.5667        0.6568  0.0140\n",
      "     76        0.6493       0.5667        0.6568  0.0150\n",
      "     77        0.6395       0.5667        0.6559  0.0125\n",
      "     78        0.6458       0.5667        0.6556  0.0140\n",
      "     79        0.6362       0.5667        0.6551  0.0165\n",
      "     80        0.6389       0.5667        0.6548  0.0095\n",
      "     81        0.6412       0.5667        0.6541  0.0125\n",
      "     82        0.6425       0.5667        0.6538  0.0130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83        0.6385       0.5667        0.6534  0.0110\n",
      "     84        0.6407       0.5667        0.6528  0.0125\n",
      "     85        0.6330       0.5667        0.6523  0.0105\n",
      "     86        0.6390       0.5667        0.6517  0.0150\n",
      "     87        0.6364       0.5667        0.6513  0.0120\n",
      "     88        0.6320       0.5667        0.6508  0.0120\n",
      "     89        0.6397       0.5667        0.6505  0.0120\n",
      "     90        0.6207       0.5667        0.6497  0.0130\n",
      "     91        0.6261       0.5667        0.6490  0.0145\n",
      "     92        0.6372       0.5667        0.6482  0.0135\n",
      "     93        0.6330       0.5667        0.6478  0.0105\n",
      "     94        0.6280       0.5667        0.6471  0.0135\n",
      "     95        0.6265       0.5667        0.6460  0.0130\n",
      "     96        0.6316       0.5667        0.6454  0.0110\n",
      "     97        0.6313       0.5667        0.6449  0.0110\n",
      "     98        0.6410       0.5667        0.6445  0.0130\n",
      "     99        0.6269       0.5667        0.6438  0.0155\n",
      "    100        0.6259       0.5667        0.6433  0.0145\n",
      "    101        0.6194       0.5667        0.6424  0.0145\n",
      "    102        0.6311       0.5667        0.6418  0.0135\n",
      "    103        0.6244       0.5667        0.6406  0.0135\n",
      "    104        0.6387       0.5667        0.6402  0.0165\n",
      "    105        0.6302       0.5667        0.6397  0.0110\n",
      "    106        0.6268       0.5667        0.6392  0.0170\n",
      "    107        0.6262       0.5667        0.6384  0.0135\n",
      "    108        0.6357       0.5667        0.6378  0.0105\n",
      "    109        0.6331       0.5667        0.6373  0.0140\n",
      "    110        0.6343       0.5667        0.6371  0.0125\n",
      "    111        0.6205       0.5667        0.6361  0.0135\n",
      "    112        0.6249       0.5667        0.6353  0.0140\n",
      "    113        0.6253       0.5667        0.6344  0.0125\n",
      "    114        0.6317       0.5667        0.6341  0.0110\n",
      "    115        0.6097       0.5667        0.6335  0.0120\n",
      "    116        0.6230       0.5667        0.6330  0.0125\n",
      "    117        0.6194       0.5667        0.6323  0.0115\n",
      "    118        0.6308       0.5667        0.6316  0.0120\n",
      "    119        0.6233       0.5667        0.6309  0.0125\n",
      "    120        0.6309       0.5667        0.6306  0.0105\n",
      "    121        0.6095       0.5667        0.6293  0.0120\n",
      "    122        0.6156       0.5667        0.6285  0.0135\n",
      "    123        0.6186       0.5667        0.6281  0.0120\n",
      "    124        0.6225       0.5667        0.6276  0.0105\n",
      "    125        0.6115       0.5667        0.6266  0.0140\n",
      "    126        0.6041       0.5667        0.6254  0.0110\n",
      "    127        0.6105       0.5667        0.6246  0.0160\n",
      "    128        0.6175       0.5667        0.6240  0.0100\n",
      "    129        0.6087       0.5667        0.6231  0.0135\n",
      "    130        0.6257       0.5667        0.6225  0.0120\n",
      "    131        0.6241       0.5667        0.6222  0.0150\n",
      "    132        0.6280       0.5667        0.6218  0.0115\n",
      "    133        0.6165       0.5667        0.6210  0.0130\n",
      "    134        0.6107       0.5667        0.6202  0.0125\n",
      "    135        0.6394       0.5667        0.6204  0.0110\n",
      "    136        0.6219       0.5667        0.6201  0.0125\n",
      "    137        0.6071       0.5667        0.6195  0.0120\n",
      "    138        0.6110       0.5667        0.6186  0.0110\n",
      "    139        0.6198       0.5667        0.6180  0.0130\n",
      "    140        0.6085       0.5667        0.6170  0.0150\n",
      "    141        0.6012       0.5667        0.6159  0.0140\n",
      "    142        0.6062       0.5667        0.6150  0.0125\n",
      "    143        0.6273       0.5667        0.6145  0.0105\n",
      "    144        0.5974       0.5667        0.6131  0.0110\n",
      "    145        0.6123       0.5667        0.6127  0.0115\n",
      "    146        0.6189       0.5667        0.6124  0.0125\n",
      "    147        0.6074       0.5667        0.6110  0.0145\n",
      "    148        0.5901       0.5889        0.6100  0.0115\n",
      "    149        0.6103       0.6000        0.6090  0.0160\n",
      "    150        0.6027       0.6000        0.6088  0.0140\n",
      "    151        0.5977       0.6000        0.6077  0.0120\n",
      "    152        0.6112       0.6000        0.6072  0.0115\n",
      "    153        0.6042       0.6000        0.6070  0.0155\n",
      "    154        0.6064       0.6000        0.6063  0.0140\n",
      "    155        0.5919       0.6000        0.6051  0.0125\n",
      "    156        0.6172       0.6000        0.6045  0.0145\n",
      "    157        0.6154       0.6000        0.6040  0.0115\n",
      "    158        0.6072       0.6222        0.6032  0.0150\n",
      "    159        0.6094       0.6222        0.6029  0.0115\n",
      "    160        0.5913       0.6333        0.6019  0.0150\n",
      "    161        0.6031       0.6444        0.6013  0.0125\n",
      "    162        0.5900       0.6444        0.6005  0.0155\n",
      "    163        0.5981       0.6444        0.6001  0.0120\n",
      "    164        0.5968       0.6444        0.5990  0.0120\n",
      "    165        0.5964       0.6444        0.5977  0.0100\n",
      "    166        0.6062       0.6444        0.5971  0.0130\n",
      "    167        0.5998       0.6444        0.5969  0.0135\n",
      "    168        0.6067       0.6444        0.5963  0.0115\n",
      "    169        0.6172       0.6444        0.5957  0.0140\n",
      "    170        0.5978       0.6444        0.5953  0.0135\n",
      "    171        0.6011       0.6444        0.5942  0.0135\n",
      "    172        0.5954       0.6444        0.5937  0.0145\n",
      "    173        0.5810       0.6444        0.5925  0.0120\n",
      "    174        0.5939       0.6444        0.5918  0.0155\n",
      "    175        0.5988       0.6444        0.5914  0.0140\n",
      "    176        0.5906       0.6444        0.5905  0.0115\n",
      "    177        0.6090       0.6556        0.5899  0.0120\n",
      "    178        0.6032       0.6556        0.5895  0.0150\n",
      "    179        0.5970       0.6556        0.5892  0.0120\n",
      "    180        0.6125       0.6556        0.5892  0.0130\n",
      "    181        0.5974       0.6444        0.5890  0.0120\n",
      "    182        0.5880       0.6778        0.5876  0.0135\n",
      "    183        0.5854       0.7000        0.5865  0.0100\n",
      "    184        0.6049       0.7222        0.5854  0.0120\n",
      "    185        0.5811       0.7222        0.5843  0.0155\n",
      "    186        0.6017       0.7222        0.5840  0.0115\n",
      "    187        0.5846       0.7222        0.5827  0.0095\n",
      "    188        0.5927       0.7222        0.5819  0.0125\n",
      "    189        0.5927       0.7222        0.5816  0.0150\n",
      "    190        0.6052       0.7222        0.5812  0.0115\n",
      "    191        0.5874       0.7222        0.5805  0.0115\n",
      "    192        0.5685       0.7222        0.5791  0.0145\n",
      "    193        0.5907       0.7556        0.5780  0.0160\n",
      "    194        0.5721       0.7556        0.5769  0.0110\n",
      "    195        0.5769       0.7556        0.5764  0.0130\n",
      "    196        0.5922       0.7667        0.5750  0.0130\n",
      "    197        0.5861       0.7667        0.5738  0.0150\n",
      "    198        0.6111       0.7667        0.5738  0.0100\n",
      "    199        0.5911       0.7667        0.5730  0.0145\n",
      "    200        0.6081       0.7667        0.5731  0.0150\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6748       0.6704        0.6561  0.0170\n",
      "      2        0.6736       0.6704        0.6546  0.0284\n",
      "      3        0.6758       0.6704        0.6534  0.0250\n",
      "      4        0.6713       0.6704        0.6520  0.0250\n",
      "      5        0.6781       0.6704        0.6506  0.0235\n",
      "      6        0.6731       0.6704        0.6495  0.0195\n",
      "      7        0.6707       0.6704        0.6484  0.0220\n",
      "      8        0.6702       0.6704        0.6475  0.0255\n",
      "      9        0.6711       0.6704        0.6467  0.0269\n",
      "     10        0.6729       0.6704        0.6458  0.0230\n",
      "     11        0.6733       0.6704        0.6451  0.0230\n",
      "     12        0.6688       0.6704        0.6443  0.0255\n",
      "     13        0.6689       0.6704        0.6437  0.0284\n",
      "     14        0.6696       0.6704        0.6431  0.0274\n",
      "     15        0.6676       0.6704        0.6424  0.0230\n",
      "     16        0.6702       0.6704        0.6419  0.0235\n",
      "     17        0.6680       0.6704        0.6413  0.0225\n",
      "     18        0.6688       0.6704        0.6407  0.0235\n",
      "     19        0.6669       0.6704        0.6401  0.0195\n",
      "     20        0.6659       0.6704        0.6396  0.0225\n",
      "     21        0.6671       0.6704        0.6391  0.0250\n",
      "     22        0.6677       0.6704        0.6386  0.0284\n",
      "     23        0.6677       0.6704        0.6380  0.0244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24        0.6613       0.6704        0.6375  0.0215\n",
      "     25        0.6668       0.6704        0.6371  0.0230\n",
      "     26        0.6695       0.6704        0.6368  0.0255\n",
      "     27        0.6639       0.6704        0.6363  0.0205\n",
      "     28        0.6628       0.6704        0.6357  0.0230\n",
      "     29        0.6657       0.6704        0.6353  0.0240\n",
      "     30        0.6667       0.6704        0.6349  0.0249\n",
      "     31        0.6686       0.6704        0.6344  0.0225\n",
      "     32        0.6656       0.6704        0.6340  0.0225\n",
      "     33        0.6663       0.6704        0.6336  0.0220\n",
      "     34        0.6668       0.6704        0.6332  0.0190\n",
      "     35        0.6614       0.6704        0.6327  0.0230\n",
      "     36        0.6646       0.6704        0.6322  0.0235\n",
      "     37        0.6615       0.6704        0.6317  0.0259\n",
      "     38        0.6627       0.6704        0.6311  0.0205\n",
      "     39        0.6620       0.6704        0.6306  0.0240\n",
      "     40        0.6599       0.6704        0.6301  0.0259\n",
      "     41        0.6639       0.6704        0.6296  0.0235\n",
      "     42        0.6547       0.6704        0.6290  0.0215\n",
      "     43        0.6520       0.6704        0.6283  0.0239\n",
      "     44        0.6535       0.6704        0.6276  0.0250\n",
      "     45        0.6573       0.6704        0.6270  0.0250\n",
      "     46        0.6559       0.6704        0.6264  0.0235\n",
      "     47        0.6545       0.6704        0.6257  0.0220\n",
      "     48        0.6520       0.6704        0.6251  0.0255\n",
      "     49        0.6478       0.6704        0.6245  0.0225\n",
      "     50        0.6531       0.6704        0.6235  0.0235\n",
      "     51        0.6524       0.6704        0.6227  0.0284\n",
      "     52        0.6506       0.6704        0.6218  0.0235\n",
      "     53        0.6482       0.6704        0.6208  0.0220\n",
      "     54        0.6487       0.6704        0.6197  0.0225\n",
      "     55        0.6481       0.6704        0.6186  0.0274\n",
      "     56        0.6481       0.6704        0.6177  0.0269\n",
      "     57        0.6470       0.6704        0.6167  0.0210\n",
      "     58        0.6482       0.6704        0.6158  0.0240\n",
      "     59        0.6447       0.6704        0.6147  0.0240\n",
      "     60        0.6443       0.6704        0.6138  0.0230\n",
      "     61        0.6445       0.6704        0.6127  0.0210\n",
      "     62        0.6429       0.6704        0.6116  0.0245\n",
      "     63        0.6441       0.6704        0.6106  0.0220\n",
      "     64        0.6405       0.6704        0.6094  0.0245\n",
      "     65        0.6341       0.6704        0.6082  0.0230\n",
      "     66        0.6431       0.6704        0.6071  0.0255\n",
      "     67        0.6368       0.6760        0.6058  0.0215\n",
      "     68        0.6330       0.6872        0.6046  0.0270\n",
      "     69        0.6307       0.6927        0.6034  0.0225\n",
      "     70        0.6396       0.6927        0.6024  0.0235\n",
      "     71        0.6240       0.6872        0.6012  0.0225\n",
      "     72        0.6283       0.6927        0.5998  0.0230\n",
      "     73        0.6329       0.6927        0.5987  0.0250\n",
      "     74        0.6335       0.6927        0.5976  0.0200\n",
      "     75        0.6280       0.6927        0.5962  0.0230\n",
      "     76        0.6315       0.6927        0.5951  0.0235\n",
      "     77        0.6279       0.6983        0.5939  0.0260\n",
      "     78        0.6280       0.6983        0.5927  0.0245\n",
      "     79        0.6275       0.6983        0.5913  0.0205\n",
      "     80        0.6137       0.7095        0.5898  0.0220\n",
      "     81        0.6268       0.7095        0.5886  0.0274\n",
      "     82        0.6274       0.7095        0.5874  0.0240\n",
      "     83        0.6374       0.7095        0.5865  0.0235\n",
      "     84        0.6186       0.7095        0.5851  0.0210\n",
      "     85        0.6155       0.7151        0.5838  0.0220\n",
      "     86        0.6245       0.7095        0.5825  0.0245\n",
      "     87        0.6298       0.7207        0.5817  0.0210\n",
      "     88        0.6188       0.7263        0.5805  0.0215\n",
      "     89        0.6093       0.7263        0.5790  0.0235\n",
      "     90        0.6237       0.7263        0.5778  0.0259\n",
      "     91        0.6147       0.7207        0.5765  0.0205\n",
      "     92        0.6205       0.7207        0.5753  0.0235\n",
      "     93        0.6118       0.7486        0.5739  0.0200\n",
      "     94        0.6095       0.7486        0.5725  0.0210\n",
      "     95        0.6191       0.7486        0.5713  0.0220\n",
      "     96        0.6128       0.7486        0.5700  0.0220\n",
      "     97        0.6236       0.7486        0.5690  0.0215\n",
      "     98        0.6130       0.7430        0.5678  0.0200\n",
      "     99        0.6068       0.7486        0.5666  0.0245\n",
      "    100        0.6037       0.7598        0.5654  0.0260\n",
      "    101        0.5898       0.7654        0.5638  0.0264\n",
      "    102        0.5982       0.7654        0.5624  0.0264\n",
      "    103        0.6152       0.7654        0.5614  0.0210\n",
      "    104        0.5911       0.7933        0.5599  0.0205\n",
      "    105        0.5923       0.8045        0.5585  0.0240\n",
      "    106        0.6017       0.8045        0.5572  0.0230\n",
      "    107        0.5988       0.8045        0.5562  0.0250\n",
      "    108        0.6007       0.8101        0.5551  0.0210\n",
      "    109        0.5972       0.8156        0.5539  0.0265\n",
      "    110        0.5888       0.8268        0.5524  0.0240\n",
      "    111        0.6017       0.8212        0.5511  0.0250\n",
      "    112        0.5905       0.8268        0.5496  0.0245\n",
      "    113        0.5973       0.8212        0.5486  0.0230\n",
      "    114        0.5809       0.8212        0.5473  0.0220\n",
      "    115        0.5955       0.8212        0.5461  0.0235\n",
      "    116        0.5892       0.8212        0.5450  0.0225\n",
      "    117        0.5837       0.8212        0.5435  0.0269\n",
      "    118        0.6003       0.8268        0.5426  0.0195\n",
      "    119        0.5817       0.8268        0.5414  0.0225\n",
      "    120        0.5717       0.8268        0.5397  0.0235\n",
      "    121        0.5966       0.8268        0.5391  0.0225\n",
      "    122        0.5866       0.8324        0.5379  0.0244\n",
      "    123        0.5832       0.8380        0.5367  0.0245\n",
      "    124        0.5771       0.8436        0.5355  0.0299\n",
      "    125        0.5894       0.8547        0.5346  0.0245\n",
      "    126        0.5870       0.8492        0.5337  0.0245\n",
      "    127        0.5652       0.8492        0.5321  0.0195\n",
      "    128        0.5931       0.8492        0.5314  0.0235\n",
      "    129        0.5855       0.8492        0.5305  0.0220\n",
      "    130        0.5702       0.8492        0.5293  0.0220\n",
      "    131        0.6084       0.8436        0.5291  0.0205\n",
      "    132        0.5752       0.8436        0.5279  0.0195\n",
      "    133        0.5784       0.8436        0.5269  0.0225\n",
      "    134        0.5918       0.8436        0.5265  0.0200\n",
      "    135        0.5818       0.8436        0.5255  0.0235\n",
      "    136        0.5775       0.8436        0.5247  0.0195\n",
      "    137        0.5638       0.8436        0.5233  0.0240\n",
      "    138        0.5810       0.8380        0.5223  0.0255\n",
      "    139        0.5730       0.8436        0.5213  0.0250\n",
      "    140        0.5719       0.8436        0.5201  0.0245\n",
      "    141        0.5732       0.8380        0.5190  0.0245\n",
      "    142        0.5792       0.8380        0.5184  0.0205\n",
      "    143        0.5693       0.8380        0.5176  0.0245\n",
      "    144        0.5627       0.8380        0.5165  0.0230\n",
      "    145        0.5752       0.8436        0.5158  0.0245\n",
      "    146        0.5728       0.8380        0.5149  0.0245\n",
      "    147        0.5531       0.8380        0.5137  0.0230\n",
      "    148        0.5760       0.8436        0.5131  0.0309\n",
      "    149        0.5590       0.8380        0.5118  0.0215\n",
      "    150        0.5585       0.8380        0.5109  0.0230\n",
      "    151        0.5706       0.8436        0.5102  0.0230\n",
      "    152        0.5574       0.8380        0.5094  0.0249\n",
      "    153        0.5478       0.8380        0.5082  0.0235\n",
      "    154        0.5585       0.8380        0.5077  0.0245\n",
      "    155        0.5859       0.8380        0.5073  0.0279\n",
      "    156        0.5559       0.8436        0.5060  0.0304\n",
      "    157        0.5482       0.8380        0.5048  0.0309\n",
      "    158        0.5496       0.8436        0.5039  0.0265\n",
      "    159        0.5450       0.8436        0.5025  0.0294\n",
      "    160        0.5529       0.8436        0.5017  0.0284\n",
      "    161        0.5470       0.8436        0.5007  0.0259\n",
      "    162        0.5665       0.8380        0.5004  0.0269\n",
      "    163        0.5582       0.8436        0.4998  0.0284\n",
      "    164        0.5499       0.8436        0.4990  0.0294\n",
      "    165        0.5753       0.8436        0.4995  0.0245\n",
      "    166        0.5692       0.8436        0.4992  0.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    167        0.5534       0.8380        0.4986  0.0250\n",
      "    168        0.5576       0.8436        0.4982  0.0245\n",
      "    169        0.5423       0.8436        0.4973  0.0250\n",
      "    170        0.5360       0.8436        0.4962  0.0235\n",
      "    171        0.5464       0.8436        0.4952  0.0235\n",
      "    172        0.5358       0.8436        0.4940  0.0235\n",
      "    173        0.5571       0.8436        0.4937  0.0235\n",
      "    174        0.5468       0.8436        0.4931  0.0255\n",
      "    175        0.5529       0.8436        0.4922  0.0250\n",
      "    176        0.5292       0.8380        0.4912  0.0255\n",
      "    177        0.5411       0.8436        0.4907  0.0235\n",
      "    178        0.5390       0.8436        0.4898  0.0235\n",
      "    179        0.5353       0.8436        0.4888  0.0245\n",
      "    180        0.5409       0.8436        0.4880  0.0250\n",
      "    181        0.5518       0.8436        0.4873  0.0255\n",
      "    182        0.5510       0.8436        0.4874  0.0250\n",
      "    183        0.5312       0.8436        0.4867  0.0190\n",
      "    184        0.5312       0.8380        0.4859  0.0235\n",
      "    185        0.5526       0.8436        0.4855  0.0249\n",
      "    186        0.5366       0.8380        0.4846  0.0210\n",
      "    187        0.5680       0.8436        0.4844  0.0230\n",
      "    188        0.5475       0.8380        0.4835  0.0205\n",
      "    189        0.5251       0.8268        0.4824  0.0225\n",
      "    190        0.5487       0.8380        0.4817  0.0269\n",
      "    191        0.5600       0.8380        0.4817  0.0250\n",
      "    192        0.5381       0.8268        0.4808  0.0225\n",
      "    193        0.5300       0.8436        0.4802  0.0245\n",
      "    194        0.5249       0.8436        0.4796  0.0255\n",
      "    195        0.5429       0.8436        0.4796  0.0220\n",
      "    196        0.5324       0.8380        0.4793  0.0220\n",
      "    197        0.5339       0.8380        0.4787  0.0235\n",
      "    198        0.5363       0.8380        0.4779  0.0220\n",
      "    199        0.5367       0.8380        0.4774  0.0240\n",
      "    200        0.5303       0.8380        0.4769  0.0260\n",
      "0.77665544332211 {'lr': 0.02, 'max_epochs': 200, 'module__num_units': 20}\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        0.6874       0.7039        0.6799  0.0255\n",
      "      2        0.6829       0.6816        0.6743  0.0304\n",
      "      3        0.6821       0.6704        0.6693  0.0240\n",
      "      4        0.6782       0.6704        0.6650  0.0225\n",
      "      5        0.6738       0.6704        0.6609  0.0230\n",
      "      6        0.6701       0.6704        0.6573  0.0240\n",
      "      7        0.6694       0.6704        0.6543  0.0265\n",
      "      8        0.6654       0.6704        0.6513  0.0269\n",
      "      9        0.6692       0.6704        0.6486  0.0220\n",
      "     10        0.6659       0.6704        0.6461  0.0240\n",
      "     11        0.6611       0.6704        0.6439  0.0215\n",
      "     12        0.6634       0.6704        0.6416  0.0260\n",
      "     13        0.6614       0.6704        0.6395  0.0215\n",
      "     14        0.6604       0.6704        0.6377  0.0284\n",
      "     15        0.6569       0.6704        0.6362  0.0230\n",
      "     16        0.6638       0.6704        0.6348  0.0230\n",
      "     17        0.6549       0.6704        0.6333  0.0255\n",
      "     18        0.6571       0.6704        0.6320  0.0240\n",
      "     19        0.6572       0.6704        0.6305  0.0230\n",
      "     20        0.6567       0.6704        0.6293  0.0200\n",
      "     21        0.6559       0.6704        0.6279  0.0245\n",
      "     22        0.6577       0.6704        0.6269  0.0195\n",
      "     23        0.6529       0.6704        0.6257  0.0230\n",
      "     24        0.6551       0.6704        0.6247  0.0235\n",
      "     25        0.6576       0.6704        0.6234  0.0240\n",
      "     26        0.6536       0.6704        0.6224  0.0255\n",
      "     27        0.6500       0.6704        0.6214  0.0255\n",
      "     28        0.6485       0.6704        0.6205  0.0250\n",
      "     29        0.6429       0.6704        0.6193  0.0255\n",
      "     30        0.6471       0.6704        0.6177  0.0220\n",
      "     31        0.6474       0.6704        0.6167  0.0260\n",
      "     32        0.6468       0.6704        0.6155  0.0255\n",
      "     33        0.6456       0.6704        0.6145  0.0220\n",
      "     34        0.6375       0.6704        0.6129  0.0240\n",
      "     35        0.6469       0.6704        0.6122  0.0260\n",
      "     36        0.6380       0.6704        0.6109  0.0225\n",
      "     37        0.6457       0.6704        0.6097  0.0230\n",
      "     38        0.6398       0.6704        0.6086  0.0215\n",
      "     39        0.6312       0.6704        0.6073  0.0170\n",
      "     40        0.6409       0.6704        0.6064  0.0205\n",
      "     41        0.6354       0.6704        0.6051  0.0190\n",
      "     42        0.6349       0.6704        0.6038  0.0230\n",
      "     43        0.6380       0.6704        0.6029  0.0220\n",
      "     44        0.6376       0.6704        0.6017  0.0235\n",
      "     45        0.6340       0.6704        0.6001  0.0230\n",
      "     46        0.6276       0.6704        0.5986  0.0249\n",
      "     47        0.6471       0.6704        0.5979  0.0210\n",
      "     48        0.6284       0.6704        0.5968  0.0265\n",
      "     49        0.6313       0.6704        0.5956  0.0235\n",
      "     50        0.6319       0.6704        0.5943  0.0215\n",
      "     51        0.6265       0.6704        0.5930  0.0205\n",
      "     52        0.6221       0.6704        0.5913  0.0220\n",
      "     53        0.6144       0.6760        0.5900  0.0274\n",
      "     54        0.6287       0.6760        0.5886  0.0210\n",
      "     55        0.6256       0.6760        0.5872  0.0220\n",
      "     56        0.6070       0.6816        0.5856  0.0200\n",
      "     57        0.6171       0.6816        0.5838  0.0230\n",
      "     58        0.6088       0.6872        0.5821  0.0230\n",
      "     59        0.6250       0.6983        0.5805  0.0200\n",
      "     60        0.6115       0.7039        0.5790  0.0190\n",
      "     61        0.6147       0.7095        0.5777  0.0190\n",
      "     62        0.6192       0.7095        0.5763  0.0230\n",
      "     63        0.6131       0.7207        0.5746  0.0250\n",
      "     64        0.5977       0.7207        0.5728  0.0215\n",
      "     65        0.6055       0.7207        0.5714  0.0255\n",
      "     66        0.6032       0.7318        0.5697  0.0240\n",
      "     67        0.6039       0.7318        0.5687  0.0200\n",
      "     68        0.6107       0.7318        0.5669  0.0215\n",
      "     69        0.6139       0.7318        0.5662  0.0210\n",
      "     70        0.6072       0.7318        0.5644  0.0225\n",
      "     71        0.6107       0.7318        0.5630  0.0269\n",
      "     72        0.5947       0.7486        0.5610  0.0225\n",
      "     73        0.5935       0.7486        0.5593  0.0245\n",
      "     74        0.6048       0.7654        0.5583  0.0235\n",
      "     75        0.5896       0.7709        0.5564  0.0240\n",
      "     76        0.5975       0.7709        0.5546  0.0240\n",
      "     77        0.5987       0.7877        0.5532  0.0235\n",
      "     78        0.6068       0.7877        0.5515  0.0225\n",
      "     79        0.5885       0.8045        0.5499  0.0245\n",
      "     80        0.5923       0.7933        0.5484  0.0230\n",
      "     81        0.5882       0.7933        0.5465  0.0260\n",
      "     82        0.6080       0.7933        0.5457  0.0229\n",
      "     83        0.5802       0.7933        0.5435  0.0274\n",
      "     84        0.5873       0.8101        0.5426  0.0230\n",
      "     85        0.5872       0.8101        0.5415  0.0250\n",
      "     86        0.5962       0.8101        0.5403  0.0260\n",
      "     87        0.5797       0.8045        0.5386  0.0304\n",
      "     88        0.5683       0.8045        0.5365  0.0225\n",
      "     89        0.6000       0.8045        0.5353  0.0225\n",
      "     90        0.5877       0.8045        0.5345  0.0235\n",
      "     91        0.5850       0.8045        0.5331  0.0250\n",
      "     92        0.5780       0.8045        0.5316  0.0195\n",
      "     93        0.5944       0.8045        0.5310  0.0230\n",
      "     94        0.5898       0.8045        0.5298  0.0255\n",
      "     95        0.5796       0.8045        0.5281  0.0220\n",
      "     96        0.5777       0.8101        0.5271  0.0250\n",
      "     97        0.5688       0.8101        0.5258  0.0205\n",
      "     98        0.5751       0.8101        0.5242  0.0215\n",
      "     99        0.5909       0.8101        0.5238  0.0225\n",
      "    100        0.5793       0.8101        0.5224  0.0220\n",
      "    101        0.5840       0.8101        0.5210  0.0215\n",
      "    102        0.5669       0.8101        0.5201  0.0250\n",
      "    103        0.5570       0.8101        0.5185  0.0269\n",
      "    104        0.5661       0.8101        0.5172  0.0230\n",
      "    105        0.5606       0.8101        0.5155  0.0259\n",
      "    106        0.5686       0.8045        0.5146  0.0245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    107        0.5557       0.8212        0.5131  0.0274\n",
      "    108        0.5600       0.8212        0.5121  0.0250\n",
      "    109        0.5585       0.8268        0.5108  0.0225\n",
      "    110        0.5592       0.8212        0.5089  0.0269\n",
      "    111        0.5648       0.8212        0.5073  0.0264\n",
      "    112        0.5494       0.8436        0.5056  0.0250\n",
      "    113        0.5690       0.8380        0.5053  0.0240\n",
      "    114        0.5704       0.8380        0.5050  0.0215\n",
      "    115        0.5583       0.8380        0.5036  0.0220\n",
      "    116        0.5451       0.8380        0.5018  0.0250\n",
      "    117        0.5537       0.8380        0.5004  0.0235\n",
      "    118        0.5651       0.8268        0.5002  0.0269\n",
      "    119        0.5580       0.8380        0.4987  0.0210\n",
      "    120        0.5492       0.8268        0.4979  0.0215\n",
      "    121        0.5618       0.8268        0.4975  0.0245\n",
      "    122        0.5500       0.8268        0.4963  0.0230\n",
      "    123        0.5571       0.8268        0.4956  0.0260\n",
      "    124        0.5459       0.8268        0.4947  0.0230\n",
      "    125        0.5445       0.8268        0.4931  0.0235\n",
      "    126        0.5411       0.8324        0.4926  0.0274\n",
      "    127        0.5495       0.8324        0.4918  0.0210\n",
      "    128        0.5421       0.8324        0.4903  0.0225\n",
      "    129        0.5523       0.8268        0.4890  0.0215\n",
      "    130        0.5373       0.8324        0.4879  0.0240\n",
      "    131        0.5282       0.8324        0.4872  0.0264\n",
      "    132        0.5268       0.8324        0.4854  0.0245\n",
      "    133        0.5326       0.8324        0.4850  0.0235\n",
      "    134        0.5588       0.8324        0.4848  0.0235\n",
      "    135        0.5321       0.8324        0.4839  0.0235\n",
      "    136        0.5392       0.8324        0.4829  0.0210\n",
      "    137        0.5421       0.8324        0.4821  0.0185\n",
      "    138        0.5414       0.8324        0.4813  0.0225\n",
      "    139        0.5201       0.8324        0.4801  0.0215\n",
      "    140        0.5425       0.8324        0.4794  0.0265\n",
      "    141        0.5366       0.8324        0.4789  0.0225\n",
      "    142        0.5198       0.8324        0.4771  0.0245\n",
      "    143        0.5412       0.8324        0.4768  0.0225\n",
      "    144        0.5294       0.8324        0.4758  0.0225\n",
      "    145        0.5314       0.8268        0.4746  0.0220\n",
      "    146        0.5399       0.8268        0.4743  0.0215\n",
      "    147        0.5406       0.8324        0.4740  0.0220\n",
      "    148        0.5260       0.8324        0.4741  0.0279\n",
      "    149        0.5302       0.8324        0.4731  0.0220\n",
      "    150        0.5256       0.8324        0.4722  0.0230\n",
      "    151        0.5346       0.8324        0.4707  0.0225\n",
      "    152        0.5223       0.8324        0.4698  0.0240\n",
      "    153        0.5432       0.8324        0.4689  0.0294\n",
      "    154        0.5316       0.8324        0.4676  0.0234\n",
      "    155        0.5271       0.8324        0.4672  0.0195\n",
      "    156        0.5413       0.8324        0.4671  0.0210\n",
      "    157        0.5248       0.8324        0.4660  0.0255\n",
      "    158        0.5249       0.8324        0.4657  0.0245\n",
      "    159        0.5257       0.8324        0.4651  0.0259\n",
      "    160        0.5181       0.8324        0.4644  0.0260\n",
      "    161        0.5354       0.8324        0.4644  0.0260\n",
      "    162        0.5139       0.8268        0.4631  0.0220\n",
      "    163        0.5293       0.8268        0.4617  0.0245\n",
      "    164        0.5372       0.8268        0.4618  0.0205\n",
      "    165        0.5518       0.8268        0.4616  0.0200\n",
      "    166        0.5397       0.8268        0.4620  0.0200\n",
      "    167        0.5190       0.8268        0.4616  0.0210\n",
      "    168        0.5286       0.8324        0.4620  0.0205\n",
      "    169        0.5233       0.8324        0.4610  0.0175\n",
      "    170        0.5333       0.8268        0.4600  0.0200\n",
      "    171        0.5119       0.8268        0.4591  0.0240\n",
      "    172        0.5121       0.8324        0.4588  0.0220\n",
      "    173        0.5177       0.8324        0.4585  0.0225\n",
      "    174        0.5080       0.8324        0.4583  0.0200\n",
      "    175        0.5235       0.8268        0.4579  0.0220\n",
      "    176        0.5229       0.8268        0.4577  0.0215\n",
      "    177        0.5179       0.8268        0.4573  0.0205\n",
      "    178        0.5279       0.8268        0.4559  0.0240\n",
      "    179        0.5247       0.8268        0.4546  0.0200\n",
      "    180        0.5360       0.8268        0.4551  0.0175\n",
      "    181        0.4885       0.8268        0.4542  0.0205\n",
      "    182        0.5120       0.8268        0.4535  0.0230\n",
      "    183        0.5128       0.8268        0.4529  0.0235\n",
      "    184        0.5251       0.8212        0.4531  0.0245\n",
      "    185        0.5023       0.8268        0.4521  0.0254\n",
      "    186        0.5201       0.8212        0.4524  0.0230\n",
      "    187        0.4980       0.8268        0.4516  0.0245\n",
      "    188        0.5055       0.8212        0.4521  0.0230\n",
      "    189        0.5331       0.8268        0.4527  0.0240\n",
      "    190        0.5100       0.8380        0.4524  0.0175\n",
      "    191        0.5184       0.8380        0.4519  0.0175\n",
      "    192        0.5082       0.8380        0.4508  0.0210\n",
      "    193        0.4871       0.8436        0.4511  0.0225\n",
      "    194        0.4942       0.8380        0.4502  0.0210\n",
      "    195        0.5171       0.8436        0.4512  0.0205\n",
      "    196        0.5317       0.8436        0.4510  0.0200\n",
      "    197        0.5260       0.8436        0.4511  0.0190\n",
      "    198        0.5031       0.8436        0.4518  0.0205\n",
      "    199        0.4775       0.8436        0.4502  0.0195\n",
      "    200        0.5106       0.8436        0.4497  0.0195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.dataset import CVSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import skorch\n",
    "print(skorch.__version__)\n",
    "\n",
    "class skorchModule(nn.Module):\n",
    "    def __init__(self,num_units=20):\n",
    "        super(skorchModule,self).__init__()\n",
    "        self.net=ClassifierModule(num_units=num_units)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X=self.net(X)\n",
    "        X=F.softmax(X,dim=-1)\n",
    "        return X\n",
    "        \n",
    "        \n",
    "model = NeuralNetClassifier(\n",
    "        skorchModule,\n",
    "        max_epochs=200,\n",
    "        lr=0.02,\n",
    "        train_split=CVSplit(5),\n",
    "        #     device='cuda',  # uncomment this to train with CUDA\n",
    "    )\n",
    "   \n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "Y = Y_train.astype(np.int64)\n",
    "\n",
    "params = {\n",
    "        'lr': [0.02],\n",
    "        'max_epochs': [200],\n",
    "        'module__num_units': [20],\n",
    "}\n",
    "\n",
    "# 用scoring='neg_log_loss'训练不正确？不知道为啥，只好在Module层加入softmax\n",
    "gs = GridSearchCV(model, params, refit=True, cv=2, scoring='accuracy')\n",
    "gs.fit(X, Y)\n",
    "print(gs.best_score_, gs.best_params_)\n",
    "gs.estimator.set_params(**gs.best_params_).fit(X, Y)\n",
    "\n",
    "predict_Survived_skorch = pd.Series(gs.predict(X_test.astype(np.float32)), name='Survived')\n",
    "skorch_result = pd.concat([IDtest, predict_Survived_skorch], axis=1)\n",
    "skorch_result.to_csv('skorch_result.csv', index=False)\n",
    "skorch_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "1、[Pytorch data_loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#)\n",
    "\n",
    "2、[Pytorch_transfer_learning_tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "3、[skorch basic usage](https://nbviewer.jupyter.org/github/dnouri/skorch/blob/master/notebooks/Basic_Usage.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
